<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Jenkins Pipeline实践分享]]></title>
    <url>%2F2018%2F07%2F24%2F2018-07-24-Jenkins_pipeline_share%2F</url>
    <content type="text"><![CDATA[前言本文为实践Jenkins Pipeline后的分享，介绍一些本人感受到的pipeline优势、创建pipeline项目的步骤以及调试运行的方法，适合建立过Jenkins项目但是没有接触过其pipeline功能的人群参考. Jenkins Pipeline介绍Jenkins Pipeline是一系列支持执行且集成持续发布pipeline到Jenkins的插件组成的子系统.持续发布pipeline指从版本控制系统中获得软件到交付给用户中间自动化的流程.每次软件更新在发布前都要经过编译打包以及多个测试和开发的场景.Jenkins Pipeline提供一组可扩展的工具通过Pipeline专用语言DSL对简单到复杂的发布Pipeline流程建模实现.Jenkins Pipeline定义成文本文件Jenkinsfile，提交到源码仓库中进行版本维护及审查.使用pipeline，用户可以获得以下优势： 代码：Pipeline以代码的形式实现，通常被检入源代码控制，使团队能够编辑，审查和迭代其传送流程。 持久化：Pipeline可以在计划和计划外重新启动Jenkins管理时同时存在。 可暂停(交互)：Pipeline可以选择停止并等待人工输入或批准，然后再继续Pipeline运行。 多功能：Pipeline支持复杂的现实世界连续交付要求，包括并行分叉/连接，循环和执行工作的能力。 可扩展：Pipeline插件支持其DSL的自定义扩展以及与其他插件集成的多个选项。 Jenkins自由风格项目先简要介绍下之前未使用pipeline时建立的Jenkins项目以做对比.新建任务-&gt;构建一个自由风格的软件项目 进入项目配置如图 在项目配置里主要有构建和构建后操作模块需要编辑，在本项目中，将参数处理/编译/代码检查/单元测试/集成测试/覆盖率生成的执行均写入shell脚本，通过构建中添加Execute shell步骤执行. 另外在build.sh后面的执行参数是在General模块中参数化构建过程中配置.构建后操作模块中需添加Publish Cppcheck results步骤处理cppcheckreport.xml生成静态代码检查报告，添加Publish Junit test result report步骤处理testreport.xml生成用例运行测试报告，添加Publish HTML reports步骤处理lcov-html目录生成覆盖率报告等.这些文件及目录均为执行shell后生成. 除了这两个模块外，还需要在源码管理模块中填写版本控制软件类型/仓库地址/用户证书/分支号等. Jenkins Pipeline项目新建任务-&gt;流水线，进入项目配置如图 主要模块流水线如图 定义中选择Pipeline script from SCM，表示Pipeline脚本使用SCM仓库中的Jenkinsfile，如果选择选项Pipeline script则会出现脚本的文本编辑框，直接在页面上进行编辑运行. 该项目中编写的Jenkinsfile (Declarative Pipeline) pipeline { agent any//指定在哪台机器上执行任务，配置Node时指定的标签名，any表示任一节点，如果出现在stage内则指定该stage任务在哪台机器执行 stages { stage(&apos;Checkout&apos;) { //定义一个下拉代码的任务 steps {//执行该任务的各个步骤 checkout([$class: &apos;GitSCM&apos;, branches: [[name: &apos;$Branch&apos;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[url: &apos;$git_root&apos;]]]) } } stage(&apos;Pre process&apos;) { steps { dir(path: &apos;jenkins/build&apos;) {//切换到某目录下执行，执行完steps会回退到原来所在目录 ... } script { ... REPORT=&quot;jenkins/report&quot; //定义工具和报告的路径变量 } } stage(&apos;Quality Analysis&apos;) { parallel {// 并行运行任务 stage(&apos;Static Analysis&apos;) { ... } stage(&apos;Unit Testing&apos;) { when {//条件判断是否执行该stage，return为真时执行 expression { return params.type == &apos;unit&apos; || params.type == &apos;all&apos; } } steps { sh &quot;./build/${params.Target} --gtest_filter=${params.uargs} --gtest_output= xml:../../../../${UNIT_REPORT}/testreport.xml&quot; //运行gtest生成测试报告 } } } stage(&apos;Code Building&apos;) { ... } stage(&apos;Integration Testing&apos;) { ... } stage(&apos;Coverage HTML Report&apos;) { ... parallel { stage(&apos;Integration Part&apos;) { ... } stage (&apos;Unit Part&apos;) { ... } } } stage(&apos;Coverage Report&apos;) { ... } stage(&apos;Deploy&apos;) { steps { input &quot;Can be deploy into product environment?&quot;//等待手动输入确认 echo &apos;TO DO Deploy&apos; } } } post {//后处理模块,根据pipeline任务执行结果进入不同分支处理,可将该模块定义在stage内单独处理其结果 aborted {//任务中断，Web界面上该任务为灰色的点 echo &apos;Pipeline WAS ABORTED&apos; } always {//始终运行分支 echo &apos;Pipeline finished&apos; ... } changed {//任务状态与上一次不一样 echo &apos;Stage HAVE CHANGED&apos; } failure {//任务失败，Web界面上该任务为红色的点 echo &apos;Stage FAILED&apos; } success {//任务成功，Web界面上该任务为蓝色的点 echo &apos;Stage was Successful&apos; } } options { buildDiscarder(logRotator(numToKeepStr: &apos;10&apos;))//可选项，表示保留最多10个构建日志. } parameters {//可配置任务参数，运行时可在Web界面上配置不同参数 string(name: &apos;git_root&apos;, defaultValue: &apos;http://gitlab.cbpmgt.com/test.git&apos;, description: &apos;git代码路径&apos;) ... string(name: &apos;Tool_Path&apos;, defaultValue: &apos;/home/jenkins/toolkit&apos;, description: &apos;Toolkit路径&apos;) } } 项目对比 Pipeline模式把自由风格模式项目中除了构建触发器外的所有模块都编码到Jenkinsfile文件中，发布配置和代码绑定在一起在SCM进行审查维护，大大增加其可移植性. Pipeline项目执行后可在项目首页形象地看到每个stage执行状态及运行时间. Pipeline日志系统简洁，当某次任务失败，需要找到其失败位置，在自由风格项目中需要点击到该任务的控制台输出，在任务执行的所有日志中查找.而Pipeline项目可直接在项目首页点击任务失败的stage看到原因. Pipeline通过input命令可实现人工介入该过程，更加可靠地判断是否需进行如发布到生产环境的后续步骤. Pipeline通过parallel命令可便捷实现并行任务，加快交付验证的速度. Pipeline开发工具命令行Pipeline LinterJenkins可以在实际运行之前从命令行Jenkins CLI验证声明式Pipeline，本项目用SSH接口运行linter.默认Jenkins的ssh接口是禁用的，需要在Jenkins管理页面上开启: 以管理员身份进入全局安全配置，在SSH Server下选择随机选取即可开启.验证ssh接口是否开启，在需要验证Jenkinsfile的机器A上执行命令获得ssh接口的ip和端口 $ curl -Lv http://$JENKINSURL/login 2&gt;&amp;1 | grep &apos;SSH&apos; &lt; X-SSH-Endpoint: 10.9.0.149:36340 则ssh端口为36340.下一步需配置认证: 将机器A上公钥内容拷贝到https://$JENKINSURL/user/$USERNAME/configure页面上的SSH Public Keys框体中，点击Apply按钮完成.验证密钥认证是否配置成功: $ ssh -v -l $USERNAME -p 36340 10.9.0.149 ...Authentication succeeded (publickey).. 即认证成功. 执行以下命令验证Jenkinsfile语法是否正确 $ ssh -l $USERNAME -p 36340 10.9.0.149 declarative-linter &lt; Jenkinsfile Jenkinsfile successfully validated. Replay调试运行当提交的Jenkinsfile执行后出错或者有其他需要更新时，可以直接使用其Replay功能基于上一次直接在页面上修改.点击上次运行的任务，左侧点击Replay，显示如下. 该功能方便快速更新当前的Pipeline过程，在未最终完成修改时无需对Jenkinsfile进行新的修改提交，最终完成后复制Main Script窗体内容到Jenkinsfile正常提交即可. Blue Ocean插件在Pipeline实践中了解到其相关的插件Blue Ocean，该插件使Pipeline实现及运行的流程更加的可视化.系统管理-&gt;管理插件-&gt;可选插件 中选择Blue Ocean进行安装安装后在自己工程中打开Open Blue Ocean并进入Blue Ocean界面，某次执行任务后视图 视图中显示的之前编写Jenkinsfile文件运行的Pipeline项目各个步骤的运行路径.Blue Ocean界面下可直接创建Pipeline项目.详见BlueOcean 创建Pipeline Pipeline Syntax除了上面的工具外，pipeline项目视图的左侧还有个Pipeline Syntax模块，里面可以在页面上填写便捷地生成很多步骤的脚本. 总结本文对比了自由风格项目与Pipeline项目的区别，重点突出了后者可视化及代码配置可维护的特点，展示pipeline项目的执行效果，最后简要介绍实践中用到的几个开发工具Blue Ocean及Replay等. 参考Jenkins Pipeline官网介绍W3C school Pipeline 介绍BlueOcean介绍]]></content>
      <categories>
        <category>工具学习</category>
      </categories>
      <tags>
        <tag>CI</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F02%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis cluster节点handshake状态问题]]></title>
    <url>%2F2018%2F06%2F15%2FTODO%2F</url>
    <content type="text"><![CDATA[问题现象运维在线上操作过程中，对21个主节点21个从节点的集群进行数据扩容,试图添加15个主节点15个从节点.添加过程中程序会遍历集群各节点执行cluster info命令检查cluster_known_nodes是否达到212+152=72. 如果超过则报错进行回滚操作(回滚时执行cluster forget新加入的节点并进行新节点下线操作).结果扩容失败，监控日志中出现该异常: Redis&lt;10.240.0.153:5067 db:0&gt;:fatal error, found [74] nodes more than we expected [72] 后面再次执行扩容操作，初始检查到节点数为43，导致43+15*2=73&gt; 72，继续报异常： [error] fatal error, found [73] nodes more than we expected [72] 查看该集群节点信息,发现有节点A(10.240.50.233:5055)和B的状态是fail和handshake. 而且不同节点上A和B的nodeid不一样，状态也有的是handshake，有的是fail. 分析查看机器监控节点A的日志，在扩容时添加节点A并启动，扩容失败时销毁. 现在的状态应该是已经下线了的. 但是看它在某节点C显示的状态为handshake，隔一会就改变一次nodeid. 通过cluster forget清除节点A信息,清除后隔一会又会出现状态为handshake且改变了nodeid的节点A信息. 这种情况看来cluster forget就无法恢复集群状态了，问题的关键还是找到触发nodeid变化的原因. 怀疑节点A销毁失败，但输入ps搜索节点A进程显示不存在. 怀疑节点A端口未关闭，仍与集群节点通信, 但netstat 查询节点A redis端口及其cluster bus端口也不存在. config set loglevel debug打开节点C的调试日志. 观察到节点C一直在尝试连接节点A的cluster bus端口然后失败. 158542:S 04 Jun 15:31:14.820 . Connecting with Node 0bcbe14df0fb83b9f4190aa115bb522121b18a5b at 10.240.50.233:15055 158542:S 04 Jun 15:31:14.820 . I/O error reading from node link: Connection refused 查看nodeid发生变化过程中间的日志 GOSSIP 0bcbe14df0fb83b9f4190aa115bb522121b18a5b 10.240.50.233:5055 master,fail 可知当收到包含节点A fail的gossip信息后节点C记录的节点A nodeid发生变化. 源码搜索代码中日志GOSSIP打印定位到clusterProcessGossipSection() void clusterProcessGossipSection(clusterMsg *hdr, clusterLink *link) { uint16_t count = ntohs(hdr-&gt;count); clusterMsgDataGossip *g = (clusterMsgDataGossip*) hdr-&gt;data.ping.gossip; clusterNode *sender = link-&gt;node ? link-&gt;node : clusterLookupNode(hdr-&gt;sender); while(count--) { uint16_t flags = ntohs(g-&gt;flags); clusterNode *node; sds ci; ci = representClusterNodeFlags(sdsempty(), flags); serverLog(LL_DEBUG,&quot;GOSSIP %.40s %s:%d %s&quot;, g-&gt;nodename, g-&gt;ip, ntohs(g-&gt;port), ci); sdsfree(ci); /* Update our state accordingly to the gossip sections */ node = clusterLookupNode(g-&gt;nodename);//节点C中记录的节点A nodeid一直在变化，与goosip报文中的不一样，所以返回node为null if (node) { …… } else { /* If it&apos;s not in NOADDR state and we don&apos;t have it, we * start a handshake process against this IP/PORT pairs. * * Note that we require that the sender of this gossip message * is a well known node in our cluster, otherwise we risk * joining another cluster. */ if (sender &amp;&amp; !(flags &amp; CLUSTER_NODE_NOADDR) &amp;&amp; !clusterBlacklistExists(g-&gt;nodename)) { clusterStartHandshake(g-&gt;ip,ntohs(g-&gt;port)); } } /* Next node */ g++; } } 搜索代码中改变nodeid定位到createClusterNode()，定位到其调用过程clusterStartHandshake()-&gt;createClusterNode()-&gt;getRandomHexChars(node-&gt;name, CLUSTER_NAMELEN)随机生成nodeid，进入handshake状态. (如果下线的节点重新上线了，与该节点成功建立连接，并在收到该节点报文后更新其nodeid为节点真正的nodeid) 故从代码中找到nodeid变化的原因，且从代码中得出handshake状态的nodeid会发生变化，fail状态的不会变化的结论，查看环境中集群状态信息变化得到验证. 解决方法既然handshake状态是由于收到fail状态信息导致的，那么只用把fail状态forget掉就可以，而且fail状态的节点A nodeid是一直不变的. 在集群的每个节点，执行cluster forget其节点包含fail状态节点的nodeid, 之后handshake状态信息也不见了，再次执行数据扩容操作，成功完成. 问题复现如何复现这种集群状态呢? 运维说之前手动执行过cluster forget操作. 找了个测试环境3主3从的集群, 选择其中一个主节点A，先forget从节点B，然后下线B，随后节点A出现B的handshake状态，其他节点显示B为fail状态. (注意如果是下线主节点，3主集群会由于达不到大多数而无法判定节点到fail状态) 通过redis-cli去循环获取节点A的cluster nodes信息，可以看到每隔15秒B的nodeid会发生变化.15秒刚好是cluster-node-timeout的配置，查看代码中clusterCron(): void clusterCron(void) { …… handshake_timeout = server.cluster_node_timeout; …... if (nodeInHandshake(node) &amp;&amp; now - node-&gt;ctime &gt; handshake_timeout) { clusterDelNode(node); continue; } …... 即当handshake持续超过节点配置的超时时间，则从该节点的nodes里删除该handshake节点.然后下次收到goosip带该节点fail的信息又开始handshake.. 并且如果在handshake期间收到goosip消息，由于handshake nodeid一直变化，依然会进入到clusterStartHandshake(),但是该函数里面执行了clusterHandshakeInProgress()判断以防止相同ip:port多次handshake. 搜索问题去网上搜索看看有没有人碰到相同问题，定位到#issues 2965-Cluster: How to remove a node in handshake state 提问者提出: Cluster scale: 512 nodes, one master have three salves connected. In the cluster one node stay in handshake state and the node has been failed down, so in cluster nodes we can see the node id changed but can not join to cluster. How to remove this handshake node from the cluster? 作者指出原因: There are only two ways this can happen: 1. You fail to send CLUSTER FORGET to all the nodes in the cluster. So eventually there are nodes that still has a clue about this other node, and it will inform the other nodes via gossip. Make sure to send CLUSTER FORGET to every single node in the cluster. 2. Or alternatively, there is an instance running in 10.15.107.150 but you said there is not. 提问者均保证第二点是没问题的，最终确认原因就是下线节点前执行的cluster forget在某些节点由于诸如网络不稳定的原因导致失败。所以只要那些失败的节点继续执行cluster forget即可，而那些标志下线节点为fail状态的就是前面执行cluster forget失败的. 最后有人提供了个脚本执行cluster forget: #echo &quot;usage: host port&quot; nodes_addrs=$(redis-cli -h $1 -p $2 cluster nodes|grep -v handshake| awk &apos;{print $2}&apos;) echo $nodes_addrs for addr in ${nodes_addrs[@]}; do host=${addr%:*} port=${addr#*:} del_nodeids=$(redis-cli -h $host -p $port cluster nodes|grep -E &apos;handshake|fail&apos;| awk &apos;{print $1}&apos;) for nodeid in ${del_nodeids[@]}; do echo $host $port $nodeid redis-cli -h $host -p $port cluster forget $nodeid done done 虽然我认为只用forget fail状态的，handshake状态会在超时后自动删除，不过这个脚本解决这个问题还是没什么毛病…话说以后有问题还是先来github搜搜吧. 参考Redis源码]]></content>
      <categories>
        <category>问题分析</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R2M客户端zk连接未关闭问题定位]]></title>
    <url>%2F2018%2F04%2F27%2F2018-04-27-R2M-zkclient-source-issue%2F</url>
    <content type="text"><![CDATA[问题现象某用户在使用R2M系统过程中,web上监控到其连接到集群上客户端使用数量一直在增加. 运维与其沟通后未能定位问题. 听用户对问题的描述及复现, 用户启动某进程,然后进程中启动线程执行任务,任务开始时新建客户端,执行大量集群读写命令,任务终止时则调用客户端的close().期望是每个任务结束后客户端关闭,但结果是客户端不能关闭,web上监控其客户端数量只有增加,没有减少. 此时客户已经在其开发环境中复现问题,并一次任务启动多个客户端,再去终止任务,通过日志查看确实已调用与启动客户端等量的close().但web依然监控到客户端增加. 问题确认与其确认使用的r2m_client是什么版本,若版本低于1.4.8则建议升级,简单的客户端使用步骤常见问题排除… 查看该显示的客户端信息是什么.阅读web展示的代码,其客户端信息是从zookeeper某clients/path中获得的,直接连接到zk上看到的与其显示的一致. 连接到redis集群上client list查看,所有与redis集群中各节点接收读写命令的连接在任务启动时会建立,并且任务终止时正常关闭.该连接为用户核心功能. 源码及日志分析新建客户端代码逻辑:指定zk地址及集群名称,客户端通过zk获得集群中节点信息,对应每个节点创建一个连接池,之后客户端命令通过key hash值对应到某节点的连接池上,从连接池中获得与redis节点的连接使用. 然后是启动线程新建zk会话,监听一个command/path,当集群节点/配置等信息发生变化时重新加载. 注册该客户端到clients/path(创建一个临时节点(EPHEMERAL),临时节点的生命周期和客户端会话绑定.也就是说,如果客户端会话失效,那么这个节点就会自动被清除掉.) 查看客户端的close(),里面依次执行了关闭zk会话,关闭redis各节点连接. 从日志中看客户端启动和终止是成对出现的,同时redis连接也正常关闭,说明每个客户端调用了close(). 关闭zk会话的代码 ZkClient::close() public void close() throws ZkInterruptedException { if (!this._closed) { LOG.debug(&quot;Closing ZkClient...&quot;); this.getEventLock().lock(); try { this.setShutdownTrigger(true); this._eventThread.interrupt(); this._eventThread.join(2000L); this._connection.close(); this._closed = true; } catch (InterruptedException var5) { throw new ZkInterruptedException(var5); } finally { this.getEventLock().unlock(); } LOG.debug(&quot;Closing ZkClient...done&quot;); } } Zkclient close执行前打印Closing ZkClient...,执行后Closing ZkClient...done.并且在上一级调用处会打印[Notify Service] close ZkNotifyService. 建议用户打开日志debug级别,并减少并发(方便查看)后复现,查看该流程是否正常. 该次日志中发现所有注册zkclient的日志均出现在Closing ZkClient...done后面. 看起来是close()在注册zkclient之前调用,与用户沟通该次运行new client和close中间没有执行命令操作,即new完client后立即close,考虑到注册zkclient是异步线程调用的,思考是否因为close()执行太快,异步线程在其结束后调用注册zkclient导致连接未关闭. 建议用户在close()前加入sleep等待1s. 结果再次运行后连接正常关闭. 向用户指出该问题原因为异步的zk连接完成前调用close()导致,用户提出zk连接尚未完成为什么与redis节点的连接是正常的. 查看代码进行确认：客户端与redis连接的处理与该注册zkclient是分开的,前者也有一次与zk建立连接获取节点信息已建立redis节点连接然后关闭zk连接的操作. 该过程也解释了Closing ZkClient...日志比用户调用close()数量多的问题. 但是用户提出在线上是在运行好几天后停止任务zk连接未关闭的,此时zk连接应该早建立完成了. 在此期间我在本机上并发去模拟该情况时也并未出现该问题,对该原因也抱有怀疑,查看close()也发现其有去结束initZkClient的线程,不应该在close完后还有注册zkclient的情况. r2m_client Close()代码: if (!initWorkingThread.isInterrupted()) { try { initWorkingThread.interrupt(); } catch (Exception e) { // e.printStackTrace(); } } 于是建议用户按线上情况模拟,即在new client后等待一段时间再去结束任务以调用close().于是复现zk连接未关闭的问题. 之后再运行一版close()前sleep的日志,该情况下zk连接能正常关闭. 查看日志,两者zkclient均建立并注册完成的,但是发现未加sleep的日志中Closing ZkClient...done的数量比Closing ZkClient…少,并且少的数量刚好是未关闭zk连接的数量,即存在进行zk连接关闭操作但是失败的情况. 至此问题定位到这两行日志中间哪个步骤出错, 首先怀疑是线程间死锁问题,建议用户将thread信息打印出来,随后观察到每次注册zkclient的线程只有一个是与其他不一样的,而刚好这个线程的zkclient能正常关闭. 询问用户控制并发的代码逻辑,用户表示为flink调度管理的,也不知道为什么这个线程与别的不一样. 查看zkclient.close()中有捕获并抛出异常ZkInterruptedException.但是在r2m_client调用zkclient.close()处却捕获并忽略该异常了. if (serviceClient != null) { try { serviceClient.unsubscribeAll(); serviceClient.close(); } catch (Exception e) { // exceptionHandle.log(e); } finally { serviceClient = null; } } 将该注释打开并添加日志,重新打包一版r2m_client发送给用户.用户运行后确实出现异常： org.I0Itec.zkclient.exception.ZkInterruptedException: java.lang.InterruptedException at org.I0Itec.zkclient.ZkClient.close(ZkClient.java:1268) at com.wangyin.rediscluster.notification.service.ZkNotifyService.close(ZkNotifyService.java:249) at com.wangyin.rediscluster.client.OriginalCacheClusterClient.close(OriginalCacheClusterClient.java:167) at com.jd.jr.smart.data.flink.r2m.R2MSink.close(R2MSink.java:131) at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117) at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:446) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:351) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:718) at java.lang.Thread.run(Unknown Source) Caused by: java.lang.InterruptedException at java.lang.Object.wait(Native Method) at java.lang.Thread.join(Unknown Source) at org.I0Itec.zkclient.ZkClient.close(ZkClient.java:1264) ... 9 more 异常定位看到zkclient代码的异常表示很尴尬,查看github zkclient代码仓库中看该处没有更新. 在网上google到Helix JIRA issue记录了该问题, helix和用户的flink都会有分布式并行调度的操作, 故该问题在flink使用zkclient上应该也是会同样复现的. Helix JIRA中提到该原因: This is probably a zkclient bug that we should never call zkclient.close() from its own event thread context. 意思是调用 zkclient.close() 的这个线程不能是_eventThread.join()的这个_eventThread. 从日志中查看确实出现的这种情况.思考用户场景如下: 先是终止任务,则该任务中的线程包括event thread收到中断,如果添加sleep,则close()在event thread中断完成后被调用,此时正常,若不添加,则close()调用join时event thread尚在运行,抛出异常. join/wait 异常堆栈代码 public final synchronized void join(long millis) throws InterruptedException { … while (isAlive()) { //thread线程未停止则进入 long delay = millis - now; if (delay &lt;= 0) { break; } wait(delay); now = System.currentTimeMillis() - base; } } } /*... * @throws InterruptedException if any thread interrupted the * current thread before or while the current thread * was waiting for a notification. The &lt;i&gt;interrupted * status&lt;/i&gt; of the current thread is cleared when * this exception is thrown. * @see java.lang.Object#notify() * @see java.lang.Object#notifyAll() */ public final native void wait(long timeout) throws InterruptedException; 看wait()说明,由于event thread未停止,调用wait,而该方法等待过程中如果本线程(同样是event thread)被中断则抛出InterruptedException 异常. 至此从源码中也能印证添加sleep与不添加sleep产生的影响,已找到原因. 至于为什么flink调度会出现调用zkclient.close()的这个线程是_eventThread.join()的这个_eventThread的场景不得而知. 解决方案在网上搜索到某解决方法如下,其对zkclient.close()做出了修改,捕获异常并对当前线程的中断状态暂存并关闭zk连接. /** * Close the client. * * @throws ZkInterruptedException */ public void close() throws ZkInterruptedException { if (LOG.isTraceEnabled()) { StackTraceElement[] calls = Thread.currentThread().getStackTrace(); LOG.trace(&quot;closing a zkclient. callStack: &quot; + Arrays.asList(calls)); } getEventLock().lock(); try { if (_connection == null || _closed) { return; } LOG.info(&quot;Closing zkclient: &quot; + ((ZkConnection) _connection).getZookeeper()); setShutdownTrigger(true); _eventThread.interrupt(); _eventThread.join(2000); _connection.close(); _closed = true; } catch (InterruptedException e) { /** * Workaround for HELIX-264: calling ZkClient#close() in its own eventThread context will * throw ZkInterruptedException and skip ZkConnection#close() */ if (_connection != null) { try { /** * ZkInterruptedException#construct() honors InterruptedException by calling * Thread.currentThread().interrupt(); clear it first, so we can safely close the * zk-connection */ Thread.interrupted(); _connection.close(); /** * restore interrupted status of current thread */ Thread.currentThread().interrupt(); } catch (InterruptedException e1) { throw new ZkInterruptedException(e1); } } } finally { getEventLock().unlock(); if (_monitor != null) { _monitor.unregister(); } LOG.info(&quot;Closed zkclient&quot;); } } 后来在github issue里也找到有zk用户提出该问题,zkclient代码还是暂时不动吧. 建议用户close前添加sleep, 避开该问题,同时new client采用单例,毕竟一个应用与zk 保持一个client就可以,与redis的连接并发多个是没问题的. 后续思考思考该用户场景,为什么其他用户未碰到 主要是大多数场景都是new client后,一直使用该client,不使用时进程也要结束了.而该用户场景是进程中启动一个任务线程,线程中new client-&gt;操作,然后终止线程,自行释放资源并调用close(),然后不断循环该过程导致web监控到该异常. 再思考为什么r2m_client的作者会把捕获那个异常注释掉,估计是觉得该操作失败的异常对于r2m集群使用没有影响,不要中断后续处理,该异常在任务结束时会频繁抛出,而且如果打印出信息会引起用户疑问,而作者考虑到异常未关闭的zk连接在进程结束后也会释放掉. R2M系统介绍R2M系统是基于开源的Redis cluster(Redis 3.0以上版本)研发的高性能的分布式缓存系统,京东金融的大部分缓存服务都是跑在R2M上,已经平稳的保障了多次双十一和618大促,性能,可靠性和数据一致性得到了充分的验证.R2M在满足业务要求的同时,也一直在优化运维的需求,提供Web化的一键部署,数据平衡（Rebalance）,水平扩容,数据迁移,监控告警,在线命令行管理,多机房切换等一系列功能.R2M系统架构上保持精简,不依赖其他团队的组件,可以在新环境快速进行独立部署. 参考R2M系统详情查看]]></content>
      <categories>
        <category>问题分析</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>zookeeper</tag>
        <tag>客户端</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Optane SSD虚拟内存(IMDT)与DRAM性能对比]]></title>
    <url>%2F2018%2F02%2F05%2F2018-02-05-Optane-SSD-Memory-Benchmark-2%2F</url>
    <content type="text"><![CDATA[背景自上次Optane SSD虚拟内存Redis性能测试后，申请了一台内存为DRAM-only 8×16 GB且其他配置(operating systems and application versions,CPUs etc.)一致的机器。 CPU: Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz 操作系统: CentOS Linux release 7.3.1611 (Core) 两台机器IMDT和非IMDT进行性能对比测试,以测试出IMDT进行内存扩展后性能是否降低, 降低的幅度. IMDT: Intel Memory Drvie Technology 场景模拟机器A: 实际内存由8条16G DDR4组成，共128G，加入两块320G 的Optane SSD(每个CPU外接一个SSD)并执行相关安装设置后，内存扩展生效，变成650G. 机器B: 实际内存由8条16G DDR4组成，共128G. Redis 4.0单实例，禁用rdb/aof及碎片整理功能 使用vire-benchmark(来自vipshop github仓库，多线程版本的redis-benchmark)进行性能测试,与Redis实例运行在同机器的同NUMA node上，使Redis实例CPU达到100%. 对vire-benchmark做了修改，增加参数-R选项(修改如下)，能依次遍历指定范围(0-2000w)数值key，即-t set -R 20000000时能依次写入key:000000000000-key:000019999999. @@ -163,7 +163,12 @@ static void randomizeClientKey(benchmark_client c) { for (i = 0; i &lt; c-&gt;randkeylen; i++) { char *p = c-&gt;randkeyptr[i]+11; - size_t r = random() % config.randomkeys_keyspacelen; + size_t r; + if (config.randomkeys == 1) { + r = random() % config.randomkeys_keyspacelen; + } else if (config.randomkeys == 2){ + r = (config.requests/config.threads_count*c-&gt;bt-&gt;id + c-&gt;bt-&gt;requests_issued-config.pipeline+i) % config.randomkeys_keyspacelen; + } size_t j; for (j = 0; j &lt; 12; j++) { @@ -465,10 +470,11 @@ static void writeHandler(aeEventLoop *el, int fd, void *privdata, int mask) { /* Initialize request when nothing was written. */ if (c-&gt;written == 0) { /* Enforce upper bound to number of requests. */ - if (bt-&gt;requests_issued++ &gt;= bt-&gt;requests) { + if (bt-&gt;requests_issued &gt;= bt-&gt;requests) { freeClient(c); return; } + bt-&gt;requests_issued += config.pipeline; /* Really initialize: randomize keys and set start time. */ if (config.randomkeys) randomizeClientKey(c); @@ -781,7 +787,7 @@ static void benchmark(char *title, char *cmd, int len) { clients_remainder = config.numclients%config.threads_count; bts = darray_create(config.threads_count, sizeof(benchmark_thread)); - for (i = 0; i &lt; config.threads_count; i ++) { + for (i = config.threads_count-1; i &gt;= 0; i--) { benchmark_thread *bt = darray_push(bts); bt-&gt;id = i; benchmark_thread_init(bt, @@ -843,9 +849,10 @@ int parseOptions(int argc, const char **argv) { if (lastarg) goto invalid; config.pipeline = atoi(argv[++i]); if (config.pipeline &lt;= 0) config.pipeline=1; - } else if (!strcmp(argv[i],&quot;-r&quot;)) { + } else if (!strcmp(argv[i],&quot;-r&quot;) || !strcmp(argv[i],&quot;-R&quot;)) { if (lastarg) goto invalid; config.randomkeys = 1; + if(!strcmp(argv[i],&quot;-R&quot;)) config.randomkeys = 2 ; config.randomkeys_keyspacelen = atoi(argv[++i]); if (config.randomkeys_keyspacelen &lt; 0) config.randomkeys_keyspacelen = 0; 测试场景如下: Case1=1K value, 依次写入1亿个key， 再依次读取 Case2=10K value,依次写入1000w个key，再依次读取 Case3=100K value,依次写入90w个key，再依次读取 注: 写入数据量接近机器B 128G内存. 测试结果All-DRAM 100GB database测试结果 Test Case Ops Tps TP99(ms) Maxi Time(ms) Case 1 Set 224530 2 61 Get 232694 1 6 Case 2 Set 122382 2 25 Get 157918 1 8 Case 3 Set 16177 8 15 Get 23134 5 13 DRAM+2 Optane SSD (IMDT) 100GB database 测试结果 Test Case Ops Tps TP99(ms) Maxi Time(ms) Case 1 Set 159887 4 191 Get 180684 2 5 Case 2 Set 90300 4 52 Get 133625 3 8 Case 3 Set 13217 24 31 Get 20917 6 30 图形化set 1k性能对比，横坐标为当前已使用内存，纵坐标为TPS. 从上面图表中看出在0-100GB数据场景下，写入new key的性能大概降至70%, 延迟波动比较大。读key的性能大概为80% 针对1k value场景进一步测试: ALL-DRAM下100GB数据写完后，随机set key模拟更新操作: Ops Tps TP99(ms) Maxi Time(ms) Set 215800 1 1 Get 229631 1 1 IMDT 600GB数据持续写入tps与已使用内存关系如下图： 从上图中可以看到IMDT下内存达到100GB和600GB时tps无差异. IMDT 600GB数据写完后，随机set key模拟更新操作: Ops Tps TP99(ms) Maxi Time(ms) Set 58377 6 7 Get 61206 6 7 即冷数据set性能为DRAM的58377/215800=27%get性能为DRAM的61206/229631=26.7% 对其中100GB数据持续访问使其变成热数据，然后测试其性能: Ops Tps TP99(ms) Maxi Time(ms) Set 184604 1 1 Get 196066 1 1 即热数据set性能为Dram的184604/215800=85.5%get性能为Dram的196066/229631=85.4% 总结 IMDT其热数据读写性能为DRAM的85%,冷数据读写性能为DRAM的27% 新key的写入应该是使用到DRAM，其写性能为DRAM的70% 热数据最大空间为机器的DRAM大小 参考Optane SSD虚拟内存Redis性能测试]]></content>
      <categories>
        <category>测试技术</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>性能测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现分割字符串]]></title>
    <url>%2F2017%2F12%2F29%2F2017-12-29-Linux-C-Split%2F</url>
    <content type="text"><![CDATA[背景遇到一个将字符串分割场景.以前从没有用c语言实现,都是使用python的split()函数,python处理起来很简单. split()方法语法： str.split(str=&quot;&quot;, num=string.count(str)). • str -- 分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。 • num -- 分割次数。 返回分割后的字符串列表。 用例: #!/usr/bin/python str = &quot;Line1-abcdef \nLine2-abc \nLine4-abcd&quot;; print str.split( ); print str.split(&apos; &apos;, 1 ); 以上实例输出结果如下： [&apos;Line1-abcdef&apos;, &apos;Line2-abc&apos;, &apos;Line4-abcd&apos;] [&apos;Line1-abcdef&apos;, &apos;\nLine2-abc \nLine4-abcd&apos;] Python split()通过指定分隔符对字符串进行切片，如果参数num 有指定值，则仅分隔 num 个子字符串 C语言扩展那使用C语言如何实现？ 网上搜索到有个strtok函数具有分割字符串功能，信息如下 # man strtok char *strtok(char *str, const char *delim); The strtok() function breaks a string into a sequence of zero or more nonempty tokens. On the first call to strtok() the string to be parsed should be specified in str. In each subsequent call that should parse the same string, str must be NULL. C语言实现python中的split函数: #include &lt;stdio.h&gt; #include &lt;string.h&gt; // 将str字符以spl分割,存于dst中，并返回子字符串数量 int split(char dst[][80], char* str, const char* spl) { int n = 0; char *result = NULL; result = strtok(str, spl); while( result != NULL ) { strcpy(dst[n++], result); result = strtok(NULL, spl); } return n; } int main() { char str[] = &quot;what is your name?&quot;; char dst[10][80]; int cnt = split(dst, str, &quot; &quot;); for (int i = 0; i &lt; cnt; i++) puts(dst[i]); return 0; } 开始时strtok第一次调用str传入需分割的字符串,返回第一段分割出来的字符串”what”的地址(以’\0’结尾)，后续调用str为NULL，依次返回后续分割字符串”is””your””name?”的地址. 到这里感觉有点奇怪，为什么后续str为NULL还能继续对”what is your name?”进行分割，它与第一次调用的关联在哪里? strtok源码分析下载c语言标准库源码: git clone git://sourceware.org/git/glibc.git cd glibc git checkout --track -b local_glibc-2.26 origin/release/2.26/master Strtok.c: char * strtok (char *s, const char *delim) { static char *olds;//初始为0，每次调用strtok_r后设置成下一次带分割字符串地址 return __strtok_r (s, delim, &amp;olds); } Strtok_r.c: #ifndef _LIBC /* Get specification. */ # include &quot;strtok_r.h&quot; # define __strtok_r strtok_r //可直接使用strtok_r()函数分割字符串 #endif char * __strtok_r (char *s, const char *delim, char **save_ptr) { char *end; if (s == NULL) //后续分割字符串时str设置为NULL，使用上次存储在static变量中的地址 s = *save_ptr; if (*s == &apos;\0&apos;) //待分割字符串已到末尾，结束并返回NULL { *save_ptr = s; return NULL; } /* Scan leading delimiters. */ s += strspn (s, delim); //跳过直到第一个非分隔符的地址或字符串结尾 if (*s == &apos;\0&apos;) //待分割字符串已到末尾，结束并返回NULL { *save_ptr = s; return NULL; } /* Find the end of the token. */ end = s + strcspn (s, delim); //跳过直到第一个分隔符地址或字符串结尾 if (*end == &apos;\0&apos;) //待分割字符串已到末尾，结束并返回最后一个分割完的字符串 { *save_ptr = end; return s; } /* Terminate the token and make *SAVE_PTR point past it. */ *end = &apos;\0&apos;; //分割好的字符串末尾置为终止符 *save_ptr = end + 1; //保存下一次待分割字符串地址 return s; //返回本次分割完的字符串 } 可以看到原来strtok()使用static变量存放下一次待分割字符串的地址,当传入str为NULL时使用该地址继续分割,即后续strtok()调用是通过static变量与第一次strtok()关联的. strtok与strtok_r手册里提到strtok()是非线程安全的: ATTRIBUTES For an explanation of the terms used in this section, see attributes(7). ┌───────────┬───────────────┬───────────────────────┐ │Interface │ Attribute │ Value │ ├───────────┼───────────────┼───────────────────────┤ │strtok() │ Thread safety │ MT-Unsafe race:strtok │ ├───────────┼───────────────┼───────────────────────┤ │strtok_r() │ Thread safety │ MT-Safe │ └───────────┴───────────────┴───────────────────────┘ 因为根据其定义，strtok()必须使用内部静态变量来记录字符串中下一个需要解析的标记的当前位置。但是，由于指示这个位置的变量只有一个，那么，在同一个程序中出现多个解析不同字符串的strtok调用时，各自的字符串的解析就会互相干扰。 POSIX定义了一个线程安全的函数——strtok_r，以此来代替strtok 带有_r的函数主要来自于UNIX下面。 所有的带有_r和不带_r的函数的区别的是：带_r的函数是线程安全的，r的意思是reentrant，可重入的 另外要注意的是strtok和strtok_r均对字符串本身做了修改，将分隔符替换成’\0’了 strtok进阶:strsep搜索strtok字符串时，在linux kernel(4.14.9 version)源码文件string.c中看到文件开始的注释里提到: * * Fri Jun 25 1999, Ingo Oeser &lt;ioe@informatik.tu-chemnitz.de&gt; * - Added strsep() which will replace strtok() soon (because strsep() is * reentrant and should be faster). Use only strsep() in new code, please. 即新code建议使用strsep替换strtok,由于前者可重入且更快…1999年时就建议用strsep()了，结果在网上一搜分割字符串还都是strtok()… 查看strsep手册及用法 #man strsep char *strsep(char **stringp, const char *delim); 用法: #include &lt;stdio.h&gt; #include &lt;string.h&gt; int main(void) { char source[] = &quot;hello, world! welcome to china!&quot;; char delim[] = &quot; ,!&quot;; char *s = strdup(source); char *token; for(token = strsep(&amp;s, delim); token != NULL; token = strsep(&amp;s, delim)) { printf(token); printf(&quot;+&quot;); } printf(&quot;\n&quot;); return 0; } //output: hello++world++welcome+to+china++ 对比strtok用法: #include &lt;stdio.h&gt; #include &lt;string.h&gt; int main(void) { char s[] = &quot;hello, world! welcome to china!&quot;; char delim[] = &quot; ,!&quot;; char *token; for(token = strtok(s, delim); token != NULL; token = strtok(NULL, delim)) { printf(token); printf(&quot;+&quot;); } printf(&quot;\n&quot;); return 0; } //output: hello+world+welcome+china+ 二者第一个参数有差别: 由于strsep()执行完需要改变字符串s的地址以指向下一次分割开始,所以传递字符串s地址的指针 二者输出也有些差别，出现连续++的打印是因为中间输出了空字符串，在strsep()中碰到连续分隔符时会认为分隔符之间有空字符串(这一点与python的split是一致的),详见strsep源码 而strtok()会使用s += strspn (s, delim);跳过直到第一个非分隔符的地址或字符串结尾,而不会分割出空字符串. strsep源码分析源码如下 char * __strsep (char **stringp, const char *delim) { char *begin, *end; begin = *stringp; if (begin == NULL) //处理为NULL情况 return NULL; /* Find the end of the token. */ end = begin + strcspn (begin, delim); //跳过直到第一个分隔符地址或字符串结尾 if (*end) //若为分隔符则将其设置为&apos;\0&apos;，并将下次开始分割的地址赋值为end+1 { /* Terminate the token and set *STRINGP past NUL character. */ *end++ = &apos;\0&apos;; *stringp = end; } else //若为字符串结尾则将下次开始分割的地址赋值为NULL,下次处理直接返回NULL /* No more delimiters; this is the last token. */ *stringp = NULL; return begin; //返回分割字符串地址 } 使用注意事项strsep(),strtok(),strtok_r()三者都需注意: 函数会对传入的第一个参数即待分割字符串进行修改,即把分隔符替换成&apos;\0&apos;,由于该原因所以参数不能指向只读字符串. 所以通常的做法是把待分割字符串拷贝一份来处理,如上面用法中strdup的使用. strspn源码分析strtok函数中使用到strspn()来定位非分隔符地址. Strspn.c: /* Return the length of the maximum initial segment of S which contains only characters in ACCEPT. */ size_t STRSPN (const char *str, const char *accept) { if (accept[0] == &apos;\0&apos;) return 0; if (__glibc_unlikely (accept[1] == &apos;\0&apos;)) //分支预测优化 { const char *a = str; for (; *str == *accept; str++); return str - a; } //测试了下分开的时间是不分开的2倍，不理解。 //从代码本身注释看memset 64个字节可以直接在此处将memset函数当内联函数展开？ /* Use multiple small memsets to enable inlining on most targets. */ unsigned char table[256]; unsigned char *p = memset (table, 0, 64); memset (p + 64, 0, 64); memset (p + 128, 0, 64); memset (p + 192, 0, 64); unsigned char *s = (unsigned char*) accept; /* Different from strcspn it does not add the NULL on the table so can avoid check if str[i] is NULL, since table[&apos;\0&apos;] will be 0 and thus stopping the loop check. */ do p[*s++] = 1; //将分隔符对应index处置1 while (*s); s = (unsigned char*) str; if (!p[s[0]]) return 0; if (!p[s[1]]) return 1; if (!p[s[2]]) return 2; if (!p[s[3]]) return 3; //宏定义PTR_ALIGN_DOWN将向下找到与4字节对齐的地址返回 //PTR_ALIGN_DOWN (0x3, 4); return 0x0 //PTR_ALIGN_DOWN (0x4, 4); return 0x4 //PTR_ALIGN_DOWN (0x5, 4); return 0x4 s = (unsigned char *) PTR_ALIGN_DOWN (s, 4); unsigned int c0, c1, c2, c3; do { s += 4; c0 = p[s[0]]; c1 = p[s[1]]; c2 = p[s[2]]; c3 = p[s[3]]; } while ((c0 &amp; c1 &amp; c2 &amp; c3) != 0); size_t count = s - (unsigned char *) str; //此处为何不直接return count+c0+c1+c2,估计涉及到编译优化吧 return (c0 &amp; c1) == 0 ? count + c0 : count + c2 + 2; } 计时统计统计strspn中耗时使用如下代码 struct timeval t_val; gettimeofday(&amp;t_val, NULL); //write your process here struct timeval t_val_end; gettimeofday(&amp;t_val_end, NULL); struct timeval t_result; timersub(&amp;t_val_end, &amp;t_val, &amp;t_result); double consume = t_result.tv_sec + (1.0 * t_result.tv_usec)/1000000; printf(&quot;end.elapsed time= %fs \n&quot;, consume); strtok源码NetBSD及微软实现来源于某博客C库源代码NetBSD及微软实现: strtok NetBSD简单粗暴地遍历，而微软通过32bytes的空间将时间复杂度降至N，比glibc中256bytes空间利用率高 C语言实现splitC语言分割字符串可使用strsep(),最好复制一份进行分割. #include&lt;stdio.h&gt; #include &lt;string.h&gt; int split(char *str, const char *delim, char dst[][80]) { char *s = strdup(str); char *token; int n = 0; for(token = strsep(&amp;s, delim); token != NULL; token = strsep(&amp;s, delim)) { strcpy(dst[n++], token); } return n; } int main(void) { char source[] = &quot;hello, world! welcome to china!&quot;; char delim[] = &quot; ,!&quot;; char dst[10][80]; int cnt = split(source, delim, dst); for (int i = 0; i &lt; cnt; i++) printf(&quot;%s\n&quot;,dst[i]); return 0; } python的split()是返回分割字符串数组的，上面实现这个返回的动态数组比较困难,用固定的dst[10][80]…限制太多 还是C++的动态数组vector好用… C++实现split使用substr()定位分隔符,vector的push_back()动态插入分割子字符串 //字符串分割函数 std::vector&lt;std::string&gt; split(std::string str,std::string pattern) { std::string::size_type pos; std::vector&lt;std::string&gt; result; str+=pattern;//扩展字符串以方便操作 int size=str.size(); for(int i=0; i&lt;size; i++) { pos=str.find(pattern,i); if(pos&lt;size) { std::string s=str.substr(i,pos-i); result.push_back(s); i=pos+pattern.size()-1; } } return result; } 总结C语言实现split()可以使用strsep(),但还是不好用，能用python处理最好，应用C++也是不错的. libc库底层的实现有很多优化的思想…拜一拜… 参考python split使用文档 线程安全——strtok VS strtok_r strtok和strsep函数详解 likely()/unlikely() macros in the Linux kernel - how do they work?benefit? 字符串分割(C++)]]></content>
      <categories>
        <category>编程之道</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Optane SSD虚拟内存Redis性能测试]]></title>
    <url>%2F2017%2F12%2F27%2F2017-12-27-Optane-SSD-Memory-Benchmark%2F</url>
    <content type="text"><![CDATA[背景对大规模使用redis作为缓存的项目，Intel提供了一种降低成本的方案: 使用Intel Optane SSD，应用其Memory Drive技术，可大大扩展系统内存，据说实际使用能达到当前主流DDR4内存的70-80%. 价格来源于京东商城2017-12-27数据: 英特尔（Intel） Optane DC P4800X傲腾 数据中心PCI-E NVMe 375GB价格为20000 RMB 戴尔（DELL） 全新盒装服务器 ECC 内存条 16G丨DDR4 2133 Mhz UDIMM 价格为1680 RMB 以扩展到512G内存价格对比: 64GB DDR4 + 2*320GB SSD = 46720 RMB //Intel产品手册说明需要这样扩展 512GB DDR4 = 53760 RMB 价格优势很不明显…可能规模销量上去了会降价吧…不过最近内存的价格也涨了近一倍… 不管怎么，总算多了个选择… 据此对其扩展后的内存做一次性能测试，期望测试出其相当于原生内存性能的xx%. 场景模拟机器实际内存由8条16G DDR4组成，共128G，加入一块320G 的Optane SSD(官方建议每个CPU外接一个SSD)并执行相关安装设置后，内存扩展生效，变成342G.（如果原内存是192G，可扩展成512G) 对比性能测试最好能再找一台320G DRAM-only其他配置(CPU/操作系统及版本等)与实验机器一致的.然而实验环境有限，只有先用这一台已由相关人员配置好的机器进行测试. 原生Redis 4.0，使用redis-benchmark测试. 猜想Redis会优先使用128G原生内存，使用完才开始使用SSD扩展的内存. 所以需要对比使用内存小于128G时（原生内存性能）与使用内存大于128G时（扩展内存性能）. 故测试场景如下： 修改redis-benchmark.c增加个-R选项，能依次遍历指定范围(0-2000w)数值key，即-t set -R 20000000时能依次写入key:000000000000-key:000019999999. 启动redis-server 端口为9001，禁止持久化. set 2000W个key value=1K redis-benchmark -R 20000000 -n 20000000 -t set -d 1000 -p 9001 (19W tps) 打入大约140G数据 redis-benchmark -P 100 -r 1000000000 -n 14000000 -t hset -d 10000 -p 9001 启动redis-server 端口为9002，禁止持久化 set 2000W个key value=1K redis-benchmark -R 20000000 -n 20000000 -t set -d 1000 -p 9002 (13.5w tps) 再次执行以上步骤结果一致，故得出结论扩展内存性能降至13.5/19~=70%左右. 对结论进行确认得出以上结论后，由于不明白其扩展内存原理，据说使用了冷热数据分离存储，不敢确定上述思路是否正确. 听说热数据仍使用原生内存，冷数据交换到optane ssd中，故尝试测试出该场景，以确认该来源不靠谱信息. 从步骤5开始，反复访问范围在10w内的数据，以期望其变热，tps从13.5w上升至19w. redis-benchmark -r 100000 -n 10000000 -t set -d 1000 -p 9002 然后就发现一个诡异问题，每次运行时tps不稳定，一会19w，一会13.5w. 问题复现概率性的问题不好分析，需要区分出其tps高和低时的条件或场景. 尝试kill掉redis-server清空内存，再次启动新redis-server，重复执行几次benchmark还是反反复复，结果不确定，有点像三体中智子对粒子对撞机的干扰. 换台对撞机B，使用自用的机器运行多次benchmark，结果很稳定. 再换台对撞机C，使用公用测试环境机器，运行多次benchmark，结果也不稳定，tps浮动40%左右. 考虑到对撞机C上有很多进程在运行，可能抢占CPU影响redis性能，top查看也显示48个CPU使用率波动比较厉害. 通过对撞机C想起CPU这个因素来了，但是实验机器对撞机A上没有别的进程在运行，top查看其32个CPU使用率均为0.运行benchmark时也均为0.？！ %Cpu0 : 0.0 us， 0.0 sy， 0.0 ni，100.0 id， 0.0 wa， 0.0 hi， 0.0 si， 0.0 st %Cpu1 : 0.0 us， 0.0 sy， 0.0 ni，100.0 id， 0.0 wa， 0.0 hi， 0.0 si， 0.0 st ... %Cpu31 : 0.0 us， 0.0 sy， 0.0 ni，100.0 id， 0.0 wa， 0.0 hi， 0.0 si， 0.0 st 10649 root 20 0 44.035g 0.043t 1596 R 100.0 12.8 1:18.33 redis-server 10660 root 20 0 212368 77160 912 S 15.6 0.0 0:10.39 redis-benchmark 这是显示有问题吧，还是使用pidstat看看进程都运行在哪个cpu. [root@dell-test ~]# pidstat | grep 10649 12:43:01 AM 0 10649 0.01 0.02 0.00 0.03 16 redis-server [root@dell-test ~]# pidstat | grep 10660 12:44:25 AM 0 10660 0.00 0.03 0.00 0.03 17 redis-benchmark redis-server运行在CPU16，redis-benchmark运行在CPU17. 再次运行benchmark，redis-server还是CPU16，而redis-benchmark却运行在CPU25了，这也是很好理解的，毕竟redis-benchmark进程重新启动了，系统重新给其指定CPU. 重复运行了几次发现当redis-benchmark运行在特定CPU如0，16，17，18时，tps能达到19w，而在CPU25时，tps降至13.5w，问题稳定复现. 问题分析首先已知CPU 0性能正常(在前面运行中能达到高tps)，先指定redis-server运行在CPU 0上 执行以下命令统计哪些cpu性能差，重复执行人次以上 [root@dell-test ~]# for i in {1..31};do echo &quot;======cpu $i======&quot;;taskset -c $i redis-benchmark -t set -p 9001 -q;done 结果显示CPU 8-16以及24-31性能差，tps在13w左右，数量刚好是总CPU的一半. 统计对撞机B中性能均一致. 统计对撞机C中性能差的CPU为0-11，24-35共24个，数量也刚好是总CPU的一半. 测试结果也显示当redis-server或redis-benchmark进程有一个运行在性能差的CPU上，其性能就变差了. 测试两个进程同在性能差的CPU上，如一个在CPU8，一个在CPU9上，然后发现性能变好了… 这时候反应过来了，其实这32个CPU中没有性能好坏之分，只是因为两个进程运行在跨NUMA node的cpu上了. NUMA架构非统一内存访问架构（英语：Non-uniform memory access，简称NUMA）是一种为多处理器的电脑设计的内存，内存访问时间取决于内存相对于处理器的位置.在NUMA下，处理器访问它自己的本地内存的速度比非本地内存（内存位于另一个处理器，或者是处理器之间共享的内存）快一些. 实验机器lscpu显示 NUMA node0 CPU(s): 0-7，16-23 NUMA node1 CPU(s): 8-15，24-31 当两个进程运行的CPU属于不同NUMA node时，跨NUMA node通信更慢导致tps下降. 查看当前redis-server运行CPU [root@dell-test ~]# pidstat | grep redis-server 03:41:11 AM 0 10742 0.06 0.20 0.00 0.26 0 redis-server 循环运行redis-benchmark while true;do redis-benchmark -t set -p 9001 -n 200000;done 在另一个窗口持续监控redis-benchmark运行CPU情况，此时redis-server始终运行在CPU0上（因为没有别的进程来抢占）while true;do pidstat | grep redis-benchmark;sleep 1;done结果 [root@dell-test ~]# while true;do pidstat | grep redis-benchmark;sleep 1;done 03:43:21 AM 0 12797 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:22 AM 0 12801 0.00 0.00 0.00 0.00 25 redis-benchmark 03:43:23 AM 0 12801 0.00 0.00 0.00 0.00 25 redis-benchmark 03:43:24 AM 0 12808 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:25 AM 0 12812 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:26 AM 0 12816 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:27 AM 0 12820 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:28 AM 0 12824 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:29 AM 0 12828 0.00 0.00 0.00 0.00 17 redis-benchmark 03:43:30 AM 0 12832 0.00 0.00 0.00 0.00 25 redis-benchmark 03:43:31 AM 0 12836 0.00 0.00 0.00 0.00 25 redis-benchmark 03:43:32 AM 0 12836 0.00 0.00 0.00 0.00 25 redis-benchmark redis-benchmark随机在CPU17和25间运行，当在CPU 17运行时，与CPU 0属于同NUMA node，此时tps达到19w，当在CPU 25运行时，与CPU 0不属于同NUMA node，此时tps降为13.5w. [root@dell-test ~]# lscpu Architecture: x86_64 CPU op-mode(s): 32-bit， 64-bit Byte Order: Little Endian CPU(s): 32 On-line CPU(s) list: 0-31 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 2 NUMA node(s): 3 Vendor ID: GenuineIntel CPU family: 6 Model: 63 Model name: Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz Stepping: 2 CPU MHz: 1522.320 BogoMIPS: 5199.27 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 20480K NUMA node0 CPU(s): 0-7，16-23 NUMA node1 CPU(s): 8-15，24-31 NUMA node2 CPU(s): 从lscpu的信息看其实实验机器是有两个CPU， 每个CPU 8核，每个核虚拟为2个逻辑核心，每个CPU对应一个NUMA node，有各自的存储控制器. 指定cpu运行结果指定两个进程(redis-benchmark和redis-server)运行在同一NUMA node的两个非CPU0的空闲CPU上，重新运行测试，运行三次，结果如下 Test No. Read/Write TPS(us) TP999(us) Maxi Time(us) 1 BSet 187451.97 4 53 BGet 217183.56 2 4 ASet 175327.86 3 95 AGet 204712.48 2 3 2 BSet 193348.80 3 40 BGet 219401.70 2 4 ASet 171744.58 3 123 AGet 172868.31 2 3 3 BSet 192226.38 3 30 BGet 217644.44 2 4 ASet 169950.97 3 103 AGet 165436.92 2 3 注: BGet/BGet为内存小于128G时结果，AGet/AGet为内存大于128G时结果. 从结果看使用内存超过原生内存时Set tps性能降低约6-10%. TP999保持一致，最大延迟时间差距比较大，大概2-3倍，而get tps性能降低约6-24%，每次运行结果浮动很大，延迟却差不多 对本次性能测试存怀疑态度，留待使用一台配置相同，全内存DRAM的机器与其进行对比 总结在此次对Intel Optane SSD虚拟内存性能测试过程中，了解到Intel该SSD虚拟化内存的黑科技，学会及应用了linux两个命令taskset以及pidstat 特别是了解了CPU的NUMA架构以及感受到了其对性能的影响，在以后性能测试中注意该问题 参考扩展内存: intel-memory-drive-technology Linux 运行进程实时监控pidstat命令详解 taskset: 让进程运行在指定的CPU NUMA架构的CPU]]></content>
      <categories>
        <category>测试技术</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>性能测试</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Jemalloc下Redis内存分析]]></title>
    <url>%2F2017%2F12%2F12%2F2017-12-12-Jemalloc-Redis-Debug%2F</url>
    <content type="text"><![CDATA[背景线上某业务Redis集群需要迁移到SWAPDB，预先做一次线下模拟场景，主要评估内存节约情况(本次迁移主要目的). 过程中碰到与预期不符的内存占用问题，经过分析对基于Jemalloc下Redis的内存使用有了更加深入理解. 注：以下场景及分析均基于64bit系统. 场景模拟目标业务Redis 3.2集群为三主三从，每个主节点使用内存约为3.4GB，从节点约为3.34GB(相差repl-backlog-size64MB大小)，Key数量为1500W，均为hash类型，且value格式均为： redis&gt; hgetall 52acb36a653d48629d6cdc667a9f12de (0)123456789012345-123456789012 (1)1 基于以上信息模拟如下数据： for i in range(0， 45000000): #key为32为uuid字符串，实际集群使用的key是全随机16进制字符串，但经过对比此差异对结果并无影响 key = str(uuid.uuid4()).replace(&apos;-&apos;，&apos;&apos;) #r为python 支持cluster接口 r.hset(key， &quot;123456789012345-123456789012&quot;， 1) 启动与业务集群一致的Redis 3.2三主三从集群，并将以上数据写入，结果如下： used_memory:3650444616 used_memory_human:3.40G 与线上内存占用一致，数据模拟正确. 启动SWAPDB三主三从集群，并将以上数据写入，通过设置ssdb-transfer-lower-limit 100禁止转存，此时期望内存与Redis 3.2集群一直，结果如下： used_memory:3126138896 used_memory_human:2.91G 内存节省约490MB…… 那么问题来了: SWAPDB是通过转存冷数据的value到SSD中节约内存的，在未转存时应与redis内存保持一致，为什么此时节约490MB内存? 考虑到SWAPDB是基于Redis 4.0做的改动，先查看是否由于Redis版本升级导致的，启动原生Redis 4.0集群对比： used_memory:3125854232 used_memory_human:2.91G 可以看到其与SWAPDB是一致的，故该处节约490MB内存是Redis 4.0版本升级导致的. Redis内存优化分析在Redis 4.0 日志中搜索关于Memory优化的commit，未找到相关说明. 考虑到Redis可能对key-value DB存储做出过优化，启动单实例(带一个从节点)写入数据，结果Redis 3.2/ Redis 4.0均使用内存大约为1.91GB. 即key-value DB存储处起码针对此次hash类型key-value未有内存优化的升级. 只能是在集群相关处有内存优化可能.而且只对比单实例与集群节点内存差距也很大： Redis 4.0: 2.91 - 1.91 GB - 64MB = 960MB Redis 3.2: 3.40 - 1.91 GB - 64MB = 1.43GB 注： 减去64MB是主节点多出来的repl-backlog-size大小 内存差距如此大肯定是跟记录1500W个key信息有关，和集群相关的数据结构包括：clusterNode/clusterLink/clusterState，数据结构源码比较简单，最后在Redis 4.0 代码的clusterState结构体里找到： uint64_t slots_keys_count[CLUSTER_SLOTS]; rax *slots_to_keys; Rax tree确实是作者新加入的数据结构，之前看过文章，了解了下其结构，但没看到作者说其在redis哪块使用，也没去代码里找，在Redis 4.0 日志中提到，说是为了修复集群慢的问题： * A new data structure， the radix tree (rax.c) was introduced into Redis in order to fix a major Redis Cluster slowdown. (Salvatore Sanfilippo) 对比Redis 3.2代码： zskiplist *slots_to_keys; 从名称上看得出来这是记录slot和key的映射关系，查看代码其调用函数getKeysInSlot()遍历该结构查询对应slot有哪些key，所以结构里存储了1500W key的信息占用了大量内存. zskiplist内存分析zskiplist由zskiplistNode链接而成，具体介绍见后续参考，Redis 3.2中zskiplistNode定义： typedef struct zskiplistNode { robj *obj; //8 bytes + obj len，Redis 4.0中改成sds ele，更节约内存 double score; //8 bytes struct zskiplistNode *backward; //8 bytes struct zskiplistLevel { struct zskiplistNode *forward; //8 bytes unsigned int span; //4 bytes } level[]; //n level } zskiplistNode; key对应的hash slot为score，key存储在obj对象之中. 查看robj及sds定义： typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; //4 bytes int refcount; //4 bytes void *ptr; //8 bytes + sds len (createEmbeddedStringObject， sdshdr8) } robj; struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; //1 byte uint8_t alloc; //1 byte unsigned char flags; //1 byte char buf[]; //key len }; 如上，每个zskiplistNode占用内存为 8+obj len +8+8+(8+4)*n=24+12*n+4+4+8+sds len=40+12*n+1+1+1+key len+1=76+12*n bytes Redis 3.2 集群节点比单实例节点大1.43GB， 平均到每个key大约为95.3 Bytes. n约为1.608，即zskiplistNode的level平均为1.608. ZskiplistNode Level计算int zslRandomLevel(void) { int level = 1; while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)) //ZSKIPLIST_P==0.25 level += 1; return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;//ZSKIPLIST_MAXLEVEL==32 } 该函数返回level期望为1.333. 基于该值算出1500W个ziplistnode占用内存为1.38GB左右.与前面的1.43GB差距为50MB，考虑到集群其他一些结构也会占用少量内存，并且Jemalloc内存分配内存是按block分配的， 一整块一整块分配，也不可能完全精确计算，该误差可认为在正常范围内. Rax tree 结构Redis 4.0 集群节点比单实例节点大1GB， 平均到每个key大约为67 Bytes. typedef struct rax { raxNode *head; uint64_t numele; uint64_t numnodes; } rax; typedef struct raxNode { uint32_t iskey:1; /* Does this node contain a key? */ uint32_t isnull:1; /* Associated value is NULL (don&apos;t store it). */ uint32_t iscompr:1; /* Node is compressed. */ uint32_t size:29; /* Number of children， or compressed string len. */ unsigned char data[]; } raxNode; 从raxNode的结构来看，由于其每个节点大小都不固定，不能像分析zskiplist内存一样每个Node占用内存基本相同来计算总体. 先对rax tree单独分析，下载rax项目rax 源码，将以下代码加入rax-test.c中: char *random_uuid( char buf[33] ) { char *p = buf; int n; for( n = 0; n &lt; 32; ++n ) { int b = rand()%16; sprintf(p， &quot;%x&quot;， b); p += 1; } *p = 0; return buf; } void uuidMemTest(){ rax *t = raxNew(); char buf[64]， guid[33]; unsigned int hashslot; int len = 34; //2 bytes(hashslot)+32 bytes(keylen) for (int i = 0; i &lt; 45000000; i++) { random_uuid(guid); hashslot = i%16384; //slot信息占两个字节加入到key头部 if(hashslot&gt;5460) continue; buf[0] = (hashslot &gt;&gt; 8) &amp; 0xff; buf[1] = hashslot &amp; 0xff; sprintf(buf+2，&quot;%s&quot;，guid); raxInsert(t，(unsigned char*)buf，len，NULL，NULL); } } 在main函数中运行uuidMemTest()，运行完毕后查看程序占用内存，结果如下： t-&gt;numel == 15001367， t-&gt;numnodes=35066975， 内存使用 1.482g t-&gt;numel为插入key的数量，t-&gt;numnodes为rax tree中建立的raxNode数量，内存使用为1.482g，远超过了预期的960MB. 查看该项目中rax_malloc.h文件中默认使用的是libc内存分配器，而Redis 3.2和4.0均使用的是jemalloc.所以先使用jemalloc替换该项目中的libc. Jemalloc替换替换步骤如下 rax_malloc.h文件中: malloc-&gt;je_malloc realloc-&gt;je_realloc free-&gt;je_free rax.h文件中加入#include &lt;jemalloc/jemalloc.h&gt; github上下载jemalloc，解压后进入目录执行: ./configure –with-jemalloc-prefix=je_ –prefix=/usr/local/jemalloc make -j8 &amp;&amp; make install echo /usr/local/jemalloc/lib &gt;&gt; /etc/ld.so.conf ldconfig Makefile文件中编译rax-test目标命令后添加-L/usr/local/jemalloc/lib -ljemalloc 替换完成，编译运行rax-test即可. Rax tree with Jemalloc 内存替换成jemalloc后运行结果: t-&gt;numel == 15001367， t-&gt;numnodes=35066975， 内存使用992.5MB. 虽然场景中redis集群节点只多了960MB大小内存，与单独插入1500W keys的rax tree占用大小992.5MB有32MB差距，但考虑到Jemalloc内存分配内存是按block分配的， 一整块一整块分配，也不可能完全精确计算，该误差在正常范围内，可认为Redis 4.0集群节点多出来的内存都是Rax tree占用的. 接下来分析下1500W keys的rax tree如何使用的900+MB内存. 数据看起来有点复杂……简化起见先对写入数据为(000000-999999)的rax tree进行分析： for (int i = 0; i &lt;= 1000000; i++) { sprintf(buf， &quot;%06d&quot;， i); raxInsert(t， (unsigned char *) buf， 6， NULL， NULL); } 运行上面代码写入数据后结果: t-&gt;numel == 1000000， t-&gt;numnodes=1111111， 内存使用24280 kb.(写入前为5848 kb) 即100w个元素(000000-999999)`rax tree`占用了24280 - 5848 kb = 18432 kb 从代码数据结构分析(详情见参考)rax tree占用，100w个元素(000000-999999)的rax tree结构如下： [node header][0-9](10 ptr) (94 bytes: 4 bytes:header+ 10 bytes:0 ~ 9 字符 + 80 bytes:10个bit64位指针) | [node header][0-9](10 ptr) * 10 940bytes | [node header][0-9](10 ptr) * 10*10 9400bytes | [node header][0-9](10 ptr) * 10 * 10 *10 94000bytes | [node header][0-9](10 ptr) * 10 * 10 * 10 * 10 940000bytes | [node header][0-9](10 ptr) * 10 * 10 * 10 * 10 * 10 = 100，000 9400000bytes | [node header] *100w 400w bytes 每个元素指向一个空 data 的raxNode，指示该节点是一个key (4bytes) 100w个插入元素 所以数据(000000-999999)插入后内存应占用以上之和又相当于 t-&gt;numnodes * 4 (每个node head大小) + (t-&gt;numnodes-100w) * 90 (每个节点字符和其指针大小) = 14444434 bytes. 很明显与实际占用18432 kb = 18874368 bytes差别很大. 内存差异原因尝试通过valgrind的memcheck找下是否有内存泄漏问题，运行如下 valgrind --track-origins=yes --suppressions=./valgrind.sup --show-reachable=no --show-possibly-lost=no --leak-check=full ./rax-test 打印如下： ==16686== 8 bytes in 1 blocks are definitely lost in loss record 1 of 3 ==16686== at 0x514864A: je_malloc (jemalloc.c:1441) ==16686== by 0x403AAB: raxNewNode (rax.c:145) 期望raxNewNode()应该是分配4 bytes(node head大小).却分配了8 bytes，debug查看调用je_malloc处也是4. 原因是jemalloc最小分配内存单元是8bytes的分配机制导致，写程序测试多次jemalloc场景并使用命令top显示验证结果如下: 14431 zj 20 0 19888 4688 2432 S 0.0 0.0 0:00.02 demo 0 次分配程序初始内存 14471 zj 20 0 30128 15032 2540 S 1.7 0.1 0:00.05 demo 1000*1024次分配4bytes 14503 zj 20 0 30128 14956 2464 S 1.0 0.1 0:00.04 demo 每次8 bytes 14540 zj 20 0 38320 23136 2452 S 1.0 0.1 0:00.05 demo 每次10 bytes 14571 zj 20 0 38320 23108 2424 S 2.0 0.1 0:00.06 demo 每次16 bytes 可以看到每次分配4bytes和8字节是一样的，每次分配10bytes和16bytes也是一样的. 所以rax tree每个node虽然size是4bytes，但是每次jemalloc需分配8字节. 故rax tree内存实际由jemalloc分配 t-&gt;numnodes*8 (每个node head) + (t-&gt;numnodes-100w)*90 (每个节点字符和其指针)=18888878 bytes~=18874368 bytes 注:jemalloc的分配机制也影响到前面的分析，计算出来的内存无法完全与实际分配的内存一致，只是在此处由于数量巨大，才明显出现差异. 1500w uuid rax tree 内存分析见Rax tree with Jemalloc 内存运行结果: t-&gt;numel == 15001367， t-&gt;numnodes=35066975， 内存使用992.5MB. 即随机数1500W个uuid插入后，占用3500W个nodes，内存大约1GB. 每个uuid key插入时，其slot信息(2个字节)加入到其头部: void slotToKeyUpdateKey(robj *key, int add) { int ret; unsigned int hashslot = keyHashSlot(key-&gt;ptr,sdslen(key-&gt;ptr)); unsigned char buf[64]; unsigned char *indexed = buf; size_t keylen = sdslen(key-&gt;ptr); if (!server.swap_mode) server.cluster-&gt;slots_keys_count[hashslot] += add ? 1 : -1; if (keylen+2 &gt; 64) indexed = zmalloc(keylen+2); indexed[0] = (hashslot &gt;&gt; 8) &amp; 0xff; indexed[1] = hashslot &amp; 0xff; memcpy(indexed+2,key-&gt;ptr,keylen); if (add) { ret = raxInsert(server.cluster-&gt;slots_to_keys,indexed,keylen+2,NULL,NULL); if (server.swap_mode &amp;&amp; ret == 1) server.cluster-&gt;slots_keys_count[hashslot] += 1; } else { ret = raxRemove(server.cluster-&gt;slots_to_keys,indexed,keylen+2,NULL); if (server.swap_mode &amp;&amp; ret == 1) server.cluster-&gt;slots_keys_count[hashslot] += -1; } if (indexed != buf) zfree(indexed); } slot 0 ~ 5460即十六进制的0x0~0x1554,则第一个节点为0~21,第二个节点为0~255,其中21对应的下层节点范围为0x0-0x54(84),此时树结构大致如下: 1 [node header][0-20/21](22 ptr) 206 bytes: 8+22+22*8 | 2 [node header][0-255](256 ptr) *21 ---- [node header][0-84](85 ptr) *1 3085 bytes: 8+256+256*8 + 8+85+85*8 | 3 [node header][0-9a-f](16 ptr) * 5461 152bytes*5461 每个节点命中1500W/5461次 | 4 [node header][0-9a-f](16 ptr) * 5461 *16 152bytes*5461*16 每个节点命中1500W/5461/16次 | 5 [node header][0-9a-f](16 ptr) * 5461 *16 *16 152bytes*5461*16*16 每个节点命中1500W/5461/16/16~=10次 | 6 这一层node不好估计... : : 35 每个ptr指向一个空data 的raxNode，指示该节点是一个key (8bytes) 1500w个插入元素 前面1-5层由于命中次数多，则其后续字符覆盖[0-9a-f]概率很高，故可以认为每个节点均拥有16个孩子节点. 到第5层时，由于每次父节点命中平均只有10次，故其后续字符部分可能只命中一次，部分可能也能命中几次，只命中一次的节点则为compress nodes[node header][27 chars](1 ptr)，重复命中的节点在第6层仍为普通节点，在第7层为compress nodes [node header][26 chars](1 ptr). 需分析命中一次和多次节点数的期望，即10个[0-9a-f]的字符重复一次和多次的期望.TODO 期望分析比较复杂，暂时没有解决方案，先分析两种极端情况: 极端情况下如果这10次每次都不一样的字符，则第6层开始均为1500W个compress nodes: [node header][27 chars](1 ptr) * 1500W (8+27+8)*1500W 则该情况下 t-&gt;numnodes=1+22+5461+5461*16+5461*16^2+1500W+1500W=31490876 206+3085+152*(5461+5461*16+5461*16^2)+43*1500W+8*1500W=991612947 bytes ~= 945.7MB 另一种极端情况为这10次每次命中同一种字符，到了第7层每次命中不一样的字符，第7层开始均为1500W个compress nodes 第6层:[node header][0-f](10 ptr) * 5461*16*16 (8+10+10*8)*5461*16*16 第7层:[node header][26 chars](1 ptr) * 1500W (8+26+8)*1500W 则该情况下 t-&gt;numnodes=1+22+5461+5461*16+5461*16^2+1500W+1500W+5461*16*16=32888892 206+3085+152*5461*(1+16+16^2)+42*1500W+8*1500W+98*5461*16*16=1113618515 bytes ~= 1062MB 从以上两种情况结果看，与实际运行的结果(节点数和内存)比较接近. 总结集群模式下，记录slot和key的映射关系的slots_to_keys会占用比较可观的内存，Redis 4.0使用rax tree替代zskiplist结构存储不仅解决了其日志中Redis Cluster slowdown问题. 同时节约了一定量的内存，本场景1500W个uuid key相应内存使用从1.43GB降至960MB. 最后从对jemalloc的分析可以看出，在每次分配每个node(4 bytes)时会分配8 bytes的内存块，此处多分配35066975(节点数)*4 bytes=133.8MB，也许rax tree还有优化空间. 参考swapdb - 基于redis改造的冷热数据存储分离数据库 Rax, an ANSI C radix tree implementation zskiplist - Redis内部数据结构详解之跳跃表 jemalloc和内存管里 jemalloc在linux上从安装到使用]]></content>
      <categories>
        <category>问题分析</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>swapdb</tag>
        <tag>jemalloc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA TestNG DataProvider问题调试]]></title>
    <url>%2F2017%2F11%2F30%2F2017-11-30-TestNG-IDEA-Debug-Dataprovider%2F</url>
    <content type="text"><![CDATA[背景记录一次IntelliJ IDEA基于TestNG测试框架使用DataProvider碰到问题的调试过程. 代码注:以下代码经过简化 @DataProvider(name = &quot;getUrls&quot;) public Object[][] getUrls() { return new Object[][]{ {&quot;www.baidu.com&quot;}, {&quot;www.google.com&quot;}, }; } @Test(dataProvider = &quot;getUrls&quot;) public void getUrl(String url) { System.out.println(&quot;GET &quot; + url); } @DataProvider(name = &quot;postUrls&quot;) public Object[][] postUrls() { return new Object[][]{ {&quot;www.baidu.com&quot;}, {&quot;www.google.com&quot;}, }; } @Test(dataProvider = &quot;postUrls&quot;) public void postUrl(String url) { System.out.println(&quot;POST &quot; + url); } 问题在IDEA中点击绿色三角箭头运行getUrl测试方法， 期望输出 GET www.baidu.com GET www.google.com 实际只输出 GET www.baidu.com 定位 代码中有另外一个测试方法postUrl使用dataProvider却能正常输出（也是通过IDEA点击绿色三角箭头运行)， 对比两者实现并无不同. 考虑到getUrls的DataProvider返回的Object[][]可能出问题 尝试将getUrl方法对应DataProvider指定到postUrls，即改成 12345@Test(dataProvider = &quot;postUrls&quot;)public void getUrl(String url)&#123; System.out.println(&quot;GET &quot;+url);&#125; 结果仍然只输出GET www.baidu.com 继续尝试将postUrl方法对应DataProvider指定到getUrls，即改成 12345@Test(dataProvider = &quot;getUrls&quot;)public void postUrl(String url)&#123; System.out.println(&quot;POST &quot;+url);&#125; 结果正常输出 12POST www.baidu.comPOST www.google.com 说明DataProvider的定义没有问题 继续将有问题的getUrl向正常的postUrl做趋近修改，将getUrl命名改成postUrl，再次运行发现正常.将名字修改成其他的， 运行后依然能正常输出. 猜测getUrl名字有问题 直接在命令行中指定getUrl方法单独运行，能正常输出. 推测IDEA工具对getUrl处理问题 调试开始对运行测试过程进行调试 环境： Testng 6.8.8 IDEA 2017.2.6 Java 1.8 首先由于运行的是测试方法， 在testng代码中找到invokeTestMethods函数,该函数在TestMethodWorker.run()中被调用. 在该函数入口加断点， debug运行， 进入其实现中. 在invokeTestMethods函数定位到调用单个方法invokeTestMethod函数代码 while (allParameterValues.hasNext()) { … invokeTestMethod …} 查看其循环条件中allParameterValues为Object[]类型迭代器，Object[][] allParameterValues.m_objects内只存放了www.baidu.com. 所以循环只执行一次，只输出了GET www.baidu.com. 定位到allParameterValues赋值处 ParameterBag bag = createParameters(testMethod, parameters, allParameterNames, null, suite, testContext, instances[0], null); ........ Iterator&lt;Object[]&gt; allParameterValues = bag.parameterHolder.parameters; Debug createParameters函数一层层进入直到handleParameters函数，该函数负责填充方法，其注释 /** * If the method has parameters, fill them in. Either by using a @DataProvider * if any was provided, or by looking up &lt;parameters&gt; in testng.xml * @return An Iterator over the values for each parameter of this * method. */ 从注释中可以确认其使用DataProvider解析参数. parameters = MethodInvocationHelper.invokeDataProvider( instance, /* a test instance or null if the dataprovider is static*/ dataProviderHolder.method, testMethod, methodParams.context, fedInstance, annotationFinder); Iterator&lt;Object[]&gt; filteredParameters = filterParameters(parameters, testMethod.getInvocationNumbers()); 继续step调试， 经过invokeDataProvider函数解析后返回的parameters是包含了预期参数的. 问题出现在后面的filterParameters函数调用，执行后filteredParameters只剩下一组参数了. 开始分析filteredParameters函数的实现， 其传入的第二个参数为List&lt;Integer&gt; list， 即元素为整型的列表， 其使用如下： /** * If numbers is empty, return parameters, otherwise, return a subset of parameters * whose ordinal number match these found in numbers. */ static private Iterator&lt;Object[]&gt; filterParameters(Iterator&lt;Object[]&gt; parameters, List&lt;Integer&gt; list) { if (list.isEmpty()) { return parameters; } else { List&lt;Object[]&gt; result = Lists.newArrayList(); int i = 0; while (parameters.hasNext()) { Object[] next = parameters.next(); if (list.contains(i)) { result.add(next); } i++; } return new ArrayIterator(result.toArray(new Object[list.size()][])); } } 分析if(list.contains(i))代码部分及结合注释可以知道， 该函数使用第二个参数对参数组进行过滤，如果list包含数字i ,则过滤出参数组索引为i的，在该问题环境代码中，list只包含一个元素0，导致第0组参数www.baidu.com被过滤出来，最终只执行了该参数的方法. 对比正常情况list应该为null， 接下来debug list在何处被错误设置的. list为testMethod.getInvocationNumbers()返回的m_invocationNumbers，在代码中搜索定位到 public XmlInclude(String n, List&lt;Integer&gt; list, int index) { m_name = n; m_invocationNumbers = list; m_index = index; } 在该函数中加入断点重新debug，在第二次执行到该处时其被赋值为只包含一个元素0的list，此时调用栈如下 &quot;main@1&quot; prio=5 tid=0x1 nid=NA runnable java.lang.Thread.State: RUNNABLE at org.testng.xml.XmlInclude.&lt;init&gt;(XmlInclude.java:37) at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:59) at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123) 本地没有IDEA的代码，在网上搜索，找到IDEARemoteTestNG.java for (XmlInclude include : aClass.getIncludedMethods()) { includes.add(new XmlInclude(include.getName(), Collections.singletonList(Integer.parseInt(myParam)), 0)); } XmlInclue函数在该处调用，并且传入参数Collections.singletonList为list,myParam值为0 (通过IDEA在debugger窗口定位到org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:59)后加入myParam监视获得） myParam赋值 private final String myParam; public IDEARemoteTestNG(String param) { myParam = param; } 调用处在RemoteTestNGStarter.java中 final IDEARemoteTestNG testNG = new IDEARemoteTestNG(param); 查看param赋值处如下 public static void main(String[] args) throws Exception { int i = 0; String param = null; String commandFileName = null; String workingDirs = null; Vector resultArgs = new Vector(); for (; i &lt; args.length; i++) { String arg = args[i]; if (arg.startsWith(&quot;@name&quot;)) { param = arg.substring(5); continue; } param通过传入参数&quot;@name0&quot;解析出来的. 打开IDEA Console界面发现命令运行打印信息： ……… org.testng.RemoteTestNGStarter @name0 ……… 那么问题来了，这个参数是从哪里来的? 打开IDEA Run/Debug Configuration界面中getUrl测试方法对应窗口， 在JDK Settings标签中的Test runner params发现写入了个0.估计是运行测试过程中不小心敲入的:) 将0删除后运行正常,至此问题得到修复 Test runner params从IDEA官网上查看该参数的说明：Arguments to be passed to the test runner 还想知道在哪里给0加上@name前缀的. 在IDEA代码中搜索@name，定位到JavaTestFrameworkRunnableState.java protected List&lt;String&gt; getNamedParams(String parameters) { return Collections.singletonList(&quot;@name&quot; + parameters); } 该函数getNamedParams添加的@name前缀，而其被调用处testng/configuration/TestNGRunnableState.java @Override protected List&lt;String&gt; getNamedParams(String parameters) { try { Integer.parseInt(parameters); return super.getNamedParams(parameters); //添加@name } catch (NumberFormatException e) { return Arrays.asList(parameters.split(&quot; &quot;)); } } 可知当Test runner params参数为数字${num}时，传入参数@name${num}.不为数字时，直接传参. 在以后使用中，可以通过设置Test runner params为数字，使其单独运行DataProvider某一组参数. 扩展查看xmlInclude函数代码 private void xmlInclude(boolean start, Attributes attributes) { if (start) { m_locations.push(Location.INCLUDE); m_currentInclude = new Include(attributes.getValue(&quot;name&quot;), attributes.getValue(&quot;invocation-numbers&quot;)); } else { String name = m_currentInclude.name; if (null != m_currentIncludedMethods) { String in = m_currentInclude.invocationNumbers; XmlInclude include; if (!Utils.isStringEmpty(in)) { include = new XmlInclude(name, stringToList(in), m_currentIncludeIndex++); 可知invocationNumbers变量对应到testng.xml文件中为invocation-numbers，即Test runner params参数为数字时相当于在testng.xml中设置invocation-numbers，所以也可以通过修改testng.xml中&lt;include name=&quot;getUrl&quot;/&gt;为&lt;include name=&quot;getUrl&quot; invocation-numbers=&quot;0&quot;/&gt;来使用.或者一次多个&lt;include name=&quot;getUrl&quot; invocation-numbers=&quot;0 2 5&quot;/&gt;. 在网上搜索invocation-numbers作用 运行DataProvider某项失败时，在`testng-failed.xml`记录各项失败索引， 使得你可以无需运行整个测试就可以快速重新运行失败的测试，或者无需运行所有参数，与以上分析一致. 最后附上IDEA自动生成的testng.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE suite SYSTEM &quot;http://testng.org/testng-1.0.dtd&quot;&gt; &lt;suite name=&quot;Default Suite&quot;&gt; &lt;test name=&quot;jpaas_web&quot;&gt; &lt;classes&gt; &lt;class name=&quot;com.jd.jr.jpaas.qa.springcloud.IntegrationTests.URLsTest&quot;&gt; &lt;methods&gt; &lt;include name=&quot;getUrl&quot;/&gt; &lt;/methods&gt; &lt;/class&gt; &lt;!-- com.jd.jr.jpaas.qa.springcloud.IntegrationTests.URLsTest --&gt; &lt;/classes&gt; &lt;/test&gt; &lt;!-- jpaas_web --&gt; &lt;/suite&gt; &lt;!-- Default Suite --&gt; 参考Run/Debug Configuration: TestNG Markdown 语法手册 （完整整理版) TestNG参数化测试-数据提供程序 @DataProvider方式]]></content>
      <categories>
        <category>问题分析</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C#监视注册表--RegNotifyChangeKeyValue]]></title>
    <url>%2F2017%2F07%2F23%2F2017-07-23-RegNotifyChangeKeyValue-C%2F</url>
    <content type="text"><![CDATA[场景：在C#窗体应用程序中监控某软件安装更新卸载时注册表的变化， 包括键值数据的创建修改删除重命名等。 关键点：使用RegNotifyChangeKeyValue函数提供通知机制 RegNotifyChangeKeyValue该API在指定注册表项属性或内容发生更改时通知调用者。 LONG WINAPI RegNotifyChangeKeyValue( _In_ HKEY hKey, _In_ BOOL bWatchSubtree, _In_ DWORD dwNotifyFilter, _In_opt_ HANDLE hEvent, _In_ BOOL fAsynchronous ); 返回值 Long，零（ERROR_SUCCESS）表示成功。其他任何值都代表一个错误代码 参数 类型及说明 hKey，要监视的一个项的句柄，或者指定一个标准项名 bWatchSubtree，TRUE（非零）表示监视子项以及指定的项 dwNotifyFilter： REG_NOTIFY_CHANGE_NAME 侦测名称的变化，以及侦测注册表的创建和删除事件 REG_NOTIFY_CHANGE_ATTRIBUTES 侦测属性的变化 REG_NOTIFY_CHANGE_LAST_SET 侦测上一次修改时间的变化 REG_NOTIFY_CHANGE_SECURITY 侦测对安全特性的改动 hEvent Long，一个事件的句柄。如fAsynchronus为False，则这里的设置会被忽略 fAsynchronus Long，如果为零，那么除非侦测到一个变化，否则函数不会返回。否则这个函数会立即返回，而且在发生变化时触发由hEvent参数指定的一个事件 源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227using Microsoft.Win32; public class MonitorWindowsReg &#123; [DllImport(&quot;advapi32.dll&quot;, EntryPoint = &quot;RegNotifyChangeKeyValue&quot;)] private static extern int RegNotifyChangeKeyValue(IntPtr hKey, bool bWatchSubtree, int dwNotifyFilter, int hEvent, bool fAsynchronus); [DllImport(&quot;advapi32.dll&quot;, EntryPoint = &quot;RegOpenKey&quot;)] private static extern int RegOpenKey(uint hKey, string lpSubKey, ref IntPtr phkResult); [DllImport(&quot;advapi32.dll&quot;, EntryPoint = &quot;RegCloseKey&quot;)] private static extern int RegCloseKey(IntPtr hKey); private static uint HKEY_CLASSES_ROOT = 0x80000000; private static uint HKEY_CURRENT_USER = 0x80000001; private static uint HKEY_LOCAL_MACHINE = 0x80000002; private static uint HKEY_USERS = 0x80000003; private static uint HKEY_PERFORMANCE_DATA = 0x80000004; private static uint HKEY_CURRENT_CONFIG = 0x80000005; private static uint HKEY_DYN_DATA = 0x80000006; private static int REG_NOTIFY_CHANGE_NAME = 0x1; private static int REG_NOTIFY_CHANGE_ATTRIBUTES = 0x2; private static int REG_NOTIFY_CHANGE_LAST_SET = 0x4; private static int REG_NOTIFY_CHANGE_SECURITY = 0x8; /// &lt;summary&gt; /// 打开的注册表句饼 /// &lt;/summary&gt; private IntPtr _OpenIntPtr = IntPtr.Zero; private RegistryKey _OpenReg; private Hashtable _Date = new Hashtable(); private Hashtable _DateKey = new Hashtable(); public string _Text; /// &lt;summary&gt; /// 监视注册表 /// &lt;/summary&gt; /// &lt;param name=&quot;MonitorKey&quot;&gt;Microsfot.Win32.RegistryKey&lt;/param&gt; public MonitorWindowsReg(RegistryKey MonitorKey) &#123; if (MonitorKey == null) throw new Exception(&quot;注册表参数不能为NULL&quot;); _OpenReg = MonitorKey; string[] _SubKey = MonitorKey.Name.Split(&apos;\\&apos;); uint _MonitorIntPrt = HKEY_CURRENT_USER; switch (_SubKey[0]) &#123; case &quot;HKEY_CLASSES_ROOT&quot;: _MonitorIntPrt = HKEY_CLASSES_ROOT; break; case &quot;HKEY_CURRENT_USER&quot;: _MonitorIntPrt = HKEY_CURRENT_USER; break; case &quot;HKEY_LOCAL_MACHINE&quot;: _MonitorIntPrt = HKEY_LOCAL_MACHINE; break; case &quot;HKEY_USERS&quot;: _MonitorIntPrt = HKEY_USERS; break; case &quot;HKEY_PERFORMANCE_DATA&quot;: _MonitorIntPrt = HKEY_PERFORMANCE_DATA; break; case &quot;HKEY_CURRENT_CONFIG&quot;: _MonitorIntPrt = HKEY_CURRENT_CONFIG; break; case &quot;HKEY_DYN_DATA&quot;: _MonitorIntPrt = HKEY_DYN_DATA; break; default: break; &#125; //_Text = MonitorKey.Name; _Text = MonitorKey.Name.Remove(0, MonitorKey.Name.IndexOf(&apos;\\&apos;) + 1); RegOpenKey(_MonitorIntPrt, _Text, ref _OpenIntPtr); &#125; public string GetRegFullName() &#123; return _OpenReg.Name; &#125; /// &lt;summary&gt; /// 开始监控 /// &lt;/summary&gt; public void Star() &#123; if (_OpenIntPtr == IntPtr.Zero) &#123; throw new Exception(&quot;不能打开的注册项！&quot;); &#125; GetOldRegData(); System.Threading.Thread _Thread = new System.Threading.Thread(new System.Threading.ThreadStart(Monitor)); StarMonitor = true; _Thread.Start(); &#125; /// &lt;summary&gt; /// 更新老的数据表 /// &lt;/summary&gt; private void GetOldRegData() &#123; _Date.Clear(); _DateKey.Clear(); string[] OldName = new String[1]; ; try &#123; OldName = _OpenReg.GetValueNames(); &#125; catch &#123; if (OldName == null || OldName[0] == null) return; &#125; for (int i = 0; i != OldName.Length; i++) &#123; _Date.Add(OldName[i], _OpenReg.GetValue(OldName[i])); &#125; string[] OldKey = _OpenReg.GetSubKeyNames(); for (int i = 0; i != OldKey.Length; i++) &#123; _DateKey.Add(OldKey[i], &quot;&quot;); &#125; &#125; /// &lt;summary&gt; /// 停止监控 /// &lt;/summary&gt; public void Stop() &#123; StarMonitor = false; RegCloseKey(_OpenIntPtr); &#125; /// &lt;summary&gt; /// 停止标记 /// &lt;/summary&gt; private bool StarMonitor = false; /// &lt;summary&gt; /// 开始监听 /// &lt;/summary&gt; public void Monitor() &#123; while (StarMonitor) &#123; //System.Threading.Thread.Sleep(1000); RegNotifyChangeKeyValue(_OpenIntPtr, false, REG_NOTIFY_CHANGE_NAME + REG_NOTIFY_CHANGE_ATTRIBUTES + REG_NOTIFY_CHANGE_LAST_SET + REG_NOTIFY_CHANGE_SECURITY, 0, false); System.Threading.Thread.Sleep(0); GetUpdate(); &#125; &#125; /// &lt;summary&gt; /// 检查数据 /// &lt;/summary&gt; private void GetUpdate() &#123; string[] NewName = new String[1]; try &#123; NewName = _OpenReg.GetValueNames(); //获取当前的名称 &#125; catch &#123; &#125;//Rename cause a IO Exception foreach (string Key in _Date.Keys) &#123; for (int i = 0; i != NewName.Length; i++) //循环比较 &#123; if (Key == NewName[i]) &#123; if (_Date[NewName[i]].ToString() != _OpenReg.GetValue(NewName[i]).ToString()) &#123; //修改 if (UpReg != null) UpReg(_OpenReg.Name + &apos;\\&apos; + NewName[i], _Date[NewName[i]], _OpenReg.Name + &apos;\\&apos; + NewName[i], _OpenReg.GetValue(NewName[i])); &#125; NewName[i] = &quot;nullbug&quot;; //标记该value已被比较, value name can be &quot;&quot; break; &#125; else if (i == NewName.Length - 1) &#123; if (UpReg != null) UpReg(_OpenReg.Name + &apos;\\&apos; + Key, _Date[Key], &quot;&quot;, &quot;&quot;); //删除 &#125; &#125; if (0 == NewName.Length) &#123; if (UpReg != null) UpReg(_OpenReg.Name + &apos;\\&apos; + Key, _Date[Key], &quot;&quot;, &quot;&quot;); //删除 &#125; &#125; for (int i = 0; i != NewName.Length; i++) //循环比较 &#123; if (NewName[i] != &quot;nullbug&quot;) //NewName[i] != null &amp;&amp; &#123; if (UpReg != null) UpReg(&quot;&quot;, &quot;&quot;, _OpenReg.Name + &apos;\\&apos; + NewName[i], _OpenReg.GetValue(NewName[i])); //添加 &#125; &#125; //subkeys string[] NewKey = new String[1]; try &#123; NewKey = _OpenReg.GetSubKeyNames(); //获取当前的名称 &#125; catch &#123; if (NewKey[0] == null) &#123; GetOldRegData(); //TODO HOW TO avoid reach here //MessageBox.Show(_OpenReg.Name + &quot; get null reg key!&quot;); return; &#125; else &#123; MessageBox.Show(_OpenReg.Name + &quot; :&quot; + NewKey); return; &#125; &#125;//Rename cause a IO Exception foreach (string Key in _DateKey.Keys) &#123; for (int i = 0; i != NewKey.Length; i++) //循环比较 &#123; if (Key == NewKey[i]) &#123; NewKey[i] = &quot;&quot;; //标记该value已被比较 break; &#125; else if (i == NewKey.Length - 1) &#123; if (UpReg != null) UpReg(_OpenReg.Name + &apos;\\&apos; + Key, &quot;_key_&quot;, &quot;&quot;, &quot;&quot;); //删除 &#125; &#125; if (0 == NewKey.Length) &#123; if (UpReg != null) UpReg(_OpenReg.Name + &apos;\\&apos; + Key, &quot;_key_&quot;, &quot;&quot;, &quot;&quot;); //删除 &#125; &#125; for (int i = 0; i != NewKey.Length; i++) //循环比较 &#123; if (NewKey[i] != &quot;&quot;) &#123; if (UpReg != null) UpReg(&quot;&quot;, &quot;&quot;, _OpenReg.Name + &apos;\\&apos; + NewKey[i], &quot;_key_&quot;); //添加 &#125; &#125; GetOldRegData(); return; &#125; Form1类构造函数中初始化：1InitialMonitor(&quot;HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Test&quot;); 各Form1类中API定义如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250// 通过注册表项路径字符串获得RegistryKey RegistryKey GetRegistryKey(string text) &#123; string[] _SubKey = text.Split(&apos;\\&apos;); Microsoft.Win32.RegistryKey _Key = Microsoft.Win32.Registry.CurrentUser; switch (_SubKey[0]) &#123; case &quot;HKEY_CLASSES_ROOT&quot;: _Key = Microsoft.Win32.Registry.ClassesRoot; break; case &quot;HKEY_CURRENT_USER&quot;: _Key = Microsoft.Win32.Registry.CurrentUser; break; case &quot;HKEY_LOCAL_MACHINE&quot;: _Key = Microsoft.Win32.Registry.LocalMachine; break; case &quot;HKEY_USERS&quot;: _Key = Microsoft.Win32.Registry.Users; break; case &quot;HKEY_CURRENT_CONFIG&quot;: _Key = Microsoft.Win32.Registry.CurrentConfig; break; default: break; &#125; _Key = _Key.OpenSubKey(String.Join(&quot;\\&quot;, _SubKey, 1, _SubKey.Length - 1)); return _Key; &#125;// 初始化监控某注册表项 public void InitialMonitor(string text) &#123; string[] tmpText = text.Split(&apos;\\&apos;); if (tmpText.Length &lt;= 1 || tmpText[1] == null || tmpText[1] == &quot;&quot;) return; //no path input // Monitor the upper level. string upperText = String.Join(&quot;\\&quot;, tmpText, 0, tmpText.Length - 1); if (TS.Count &gt; 100) &#123; MessageBox.Show(&quot;Monitor more than 100 keys!&quot;); return; &#125; if (TS[upperText] != null) &#123; return; &#125; Microsoft.Win32.RegistryKey _Key = GetRegistryKey(upperText); if (_Key == null) &#123; MessageBox.Show(text + &quot; is not vaild path!&quot;); return; &#125; MonitorWindowsReg tmpT = new MonitorWindowsReg(_Key);// 实例化委托， 调用UpdateReg实际将调用T__UpdateReg tmpT.UpReg += new MonitorWindowsReg.UpdataReg(T__UpdateReg); tmpT.Star(); TS.Add(upperText, tmpT); // Monitor the current level recur. CreateMonitor(text); &#125;// 新增监控注册表项 public void CreateMonitor(string text) &#123; string[] tmpText = text.Split(&apos;\\&apos;); //Trace.Assert(tmpText[1] != null); if (tmpText.Length &lt;=1|| tmpText[1] == null || tmpText[1] == &quot;&quot;) return; //no path input if (TS.Count &gt; 100) &#123; MessageBox.Show(&quot;Monitor more than 100 keys!&quot;); return; &#125; if (TS[text] != null) &#123; return; &#125; Microsoft.Win32.RegistryKey _Key = GetRegistryKey(text); if(_Key == null) &#123; //MessageBox.Show(text + &quot; is not vaild path!&quot;); return; &#125; MonitorWindowsReg tmpT = new MonitorWindowsReg(_Key); tmpT.UpReg += new MonitorWindowsReg.UpdataReg(T__UpdateReg); tmpT.Star(); TS.Add(text, tmpT); foreach (string key in _Key.GetSubKeyNames()) &#123; CreateMonitor(text + &apos;\\&apos; + key); &#125; &#125;// 删除监控的注册表项 public void RemoveMonitor(string text) &#123; if (TS[text] == null) return; Microsoft.Win32.RegistryKey _Key = GetRegistryKey(text); if(_Key!=null) foreach (string key in _Key.GetSubKeyNames()) &#123; RemoveMonitor(text + &apos;\\&apos; + key); &#125; try &#123; ((MonitorWindowsReg)TS[text]).Stop(); &#125; catch &#123; &#125; TS.Remove(text); &#125;// 其他非UI线程数据传入UI线程处理 delegate void RegAppendTextCallback(string text); private void RegAppendText(string text) &#123; // InvokeRequired required compares the thread ID of the // calling thread to the thread ID of the creating thread. // If these threads are different, it returns true. if (this.richTextBox_Reg.InvokeRequired) &#123; RegAppendTextCallback d = new RegAppendTextCallback(RegAppendText); this.Invoke(d, new object[] &#123; text &#125;); &#125; else &#123; this.richTextBox_Reg.AppendText(DateTime.Now.ToLocalTime().ToString()+&apos;:&apos;+text); &#125; &#125;// 递归遍历并输出该项下面key和value的变化 void recurAppendNewKeyValue(string NewKey) &#123; Microsoft.Win32.RegistryKey _Key = GetRegistryKey(NewKey); if (_Key == null) &#123; //MessageBox.Show(text + &quot; is not vaild path!&quot;); return; &#125; RegAppendText(&quot;Create new key: &quot; + NewKey + &apos;\n&apos;); foreach (string value in _Key.GetValueNames()) &#123; RegAppendText(&quot;Create new value: &quot; + value + &quot; with data &quot; + _Key.GetValue(value).ToString() + &apos;\n&apos;); &#125; foreach (string key in _Key.GetSubKeyNames()) &#123; recurAppendNewKeyValue(NewKey + &apos;\\&apos; + key); &#125; &#125;//与MonitorWindowsReg.UpdataReg绑定 void T__UpdateReg(string OldText, object OldValue, string NewText, object NewValue) &#123; object Old = OldValue; object New = NewValue; if (Old == null || Old.ToString() == &quot;&quot;) Old = &quot;null&quot;; if (New == null || New.ToString() == &quot;&quot;) New = &quot;null&quot;; string[] nameParts = NewText.Split(&apos;\\&apos;); if(nameParts.Length&gt;2 &amp;&amp; nameParts[nameParts.Length-1] == &quot;&quot;) NewText = String.Join(&quot;\\&quot;, nameParts, 0, nameParts.Length-1); //create value if (OldText == &quot;&quot; &amp;&amp; NewText != &quot;&quot;) &#123; if (NewValue == null) &#123; if(TS[NewText] != null) &#123; RegAppendText(&quot;Delete old key: &quot; + NewText + &apos;\n&apos;); RemoveMonitor(NewText); &#125; return; &#125; if(NewValue.ToString() == &quot;_key_&quot;) &#123; recurAppendNewKeyValue(NewText); CreateMonitor(NewText); &#125; else &#123; RegAppendText(&quot;Create new value: &quot; + NewText + &quot; with data &quot; + New.ToString() + &apos;\n&apos;); &#125; &#125; else if (OldText != &quot;&quot; &amp;&amp; NewText == &quot;&quot;) &#123; if (OldValue == null) return; if (OldValue.ToString() == &quot;_key_&quot;) &#123; if(TS[OldText] != null) &#123; RegAppendText(&quot;Delete old key: &quot; + OldText + &apos;\n&apos;); RemoveMonitor(OldText); &#125; &#125; else &#123; RegAppendText(&quot;Delete old value: &quot; + OldText + &apos;\n&apos;); &#125; &#125; else if (OldText != &quot;&quot; &amp;&amp; NewText == OldText) &#123; RegAppendText(&quot;Update data of &quot;+NewText+ &quot; from &quot;+ Old.ToString() + &quot; to &quot;+ New.ToString() + &apos;\n&apos;); &#125;else &#123; RegAppendText(&quot;NewText: &quot; + NewText + &quot;OldText: &quot; + OldText + &quot; OldValue: &quot; + Old.ToString() + &quot; NewValue: &quot; + New.ToString() + &apos;\n&apos;); &#125; &#125;//停止所有监控 private void button_StopMon(object sender, EventArgs e) &#123; if (TS.Count == 0) &#123; MessageBox.Show(&quot;Register text not been monitoring!&quot;); return; &#125; foreach (string Key in new List&lt;object&gt;(TS.Keys.Cast&lt;object&gt;())) //foreach (string Key in TS.Keys)//error: Collection was modified; enumeration operation may not exec &#123; RemoveMonitor(Key); &#125; Trace.Assert(TS.Count == 0); for (int i = 0; i &lt; TextBoxes.Count; i++) //foreach (TextBox textBox in TextBoxes) &#123; ComboBoxes[i].Enabled = true; TextBoxes[i].Enabled = true; &#125; &#125;// 开启监控，并记录在hashtable TS中 private void button_StartMon(object sender, EventArgs e) &#123; if (TS.Count &gt; 0) &#123; MessageBox.Show(&quot;Register text has already been monitoring!&quot;); return; &#125; for (int i = 0; i &lt; TextBoxes.Count; i++) //foreach (TextBox textBox in TextBoxes) &#123; InitialMonitor(ComboBoxes[i].SelectedItem.ToString() + TextBoxes[i].Text); &#125; if (TS.Count &gt; 0) &#123; for (int i = 0; i &lt; TextBoxes.Count; i++) &#123; ComboBoxes[i].Enabled = false; TextBoxes[i].Enabled = false; &#125; &#125;else &#123; MessageBox.Show(&quot;No vaild path has been monitoring!&quot;); &#125; &#125; 其他本文中RegNotifyChangeKeyValue中bWatchSubtree设置为False， 自行递归监控每个子项， 每项建立一个监控。可以把bWatchSubtree设置为True，直接递归监控注册表项下所有子项。 参考C#委托(Delegate) RegNotifyChangeKeyValue function.aspx) How to: Make Thread-Safe Calls to Windows Forms Controls]]></content>
      <categories>
        <category>编程之道</category>
      </categories>
      <tags>
        <tag>C#</tag>
        <tag>windows</tag>
        <tag>注册表监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Netcat 命令----网络工具中的瑞士军刀]]></title>
    <url>%2F2017%2F07%2F16%2F2017-07-16-netcat-linux%2F</url>
    <content type="text"><![CDATA[转载自Linux Netcat 命令——网络工具中的瑞士军刀 netcat是网络工具中的瑞士军刀，它能通过TCP和UDP在网络中读写数据。通过与其他工具结合和重定向，你可以在脚本中以多种方式使用它。使用netcat命令所能完成的事情令人惊讶。 netcat所做的就是在两台电脑之间建立链接并返回两个数据流，在这之后所能做的事就看你的想像力了。你能建立一个服务器，传输文件，与朋友聊天，传输流媒体或者用它作为其它协议的独立客户端。 下面是一些使用netcat的例子. [机器A(172.31.100.7) 机器B(172.31.100.23)] 端口扫描端口扫描经常被系统管理员和黑客用来发现在一些机器上开放的端口，帮助他们识别系统中的漏洞。 $nc -z -v -n 172.31.100.7 21-25 可以运行在TCP或者UDP模式，默认是TCP，-u参数调整为udp.z 参数告诉netcat使用0 IO,连接成功后立即关闭连接， 不进行数据交换(谢谢@jxing 指点) v 参数指使用冗余选项（译者注：即详细输出） n 参数告诉netcat 不要使用DNS反向查询IP地址的域名 这个命令会打印21到25 所有开放的端口。Banner是一个文本，Banner是一个你连接的服务发送给你的文本信息。当你试图鉴别漏洞或者服务的类型和版本的时候，Banner信息是非常有用的。但是，并不是所有的服务都会发送banner。 一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。 $ nc -v 172.31.100.7 21 netcat 命令会连接开放端口21并且打印运行在这个端口上服务的banner信息。 聊天服务假如你想和你的朋友聊聊，有很多的软件和信息服务可以供你使用。但是，如果你没有这么奢侈的配置，比如你在计算机实验室，所有的对外的连接都是被限制的，你怎样和整天坐在隔壁房间的朋友沟通那？不要郁闷了，netcat提供了这样一种方法，你只需要创建一个Chat服务器，一个预先确定好的端口，这样子他就可以联系到你了。 Server $nc -l 1567 netcat 命令在1567端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示。 Client $nc 172.31.100.7 1567 不管你在机器B上键入什么都会出现在机器A上。 文件传输大部分时间中，我们都在试图通过网络或者其他工具传输文件。有很多种方法，比如FTP,SCP,SMB等等，但是当你只是需要临时或者一次传输文件，真的值得浪费时间来安装配置一个软件到你的机器上嘛。假设，你想要传一个文件file.txt 从A 到B。A或者B都可以作为服务器或者客户端，以下，让A作为服务器，B为客户端。 Server $nc -l 1567 &lt; file.txt Client $nc -n 172.31.100.7 1567 &gt; file.txt 这里我们创建了一个服务器在A上并且重定向netcat的输入为文件file.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。在客户端我们重定向输出到file.txt，当B连接到A，A发送文件内容，B保存文件内容到file.txt. 没有必要创建文件源作为Server，我们也可以相反的方法使用。像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。 B作为Server Server $nc -l 1567 &gt; file.txt Client nc 172.31.100.23 1567 &lt; file.txt 目录传输发送一个文件很简单，但是如果我们想要发送多个文件，或者整个目录，一样很简单，只需要使用压缩工具tar，压缩后发送压缩包。 如果你想要通过网络传输一个目录从A到B。 Server $tar -cvf – dir_name | nc -l 1567 Client $nc -n 172.31.100.7 1567 | tar -xvf - 这里在A服务器上，我们创建一个tar归档包并且通过-在控制台重定向它，然后使用管道，重定向给netcat，netcat可以通过网络发送它。在客户端我们下载该压缩包通过netcat 管道然后打开文件。 如果想要节省带宽传输压缩包，我们可以使用bzip2或者其他工具压缩。 Server $tar -cvf – dir_name| bzip2 -z | nc -l 1567 通过bzip2压缩 Client $nc -n 172.31.100.7 1567 | bzip2 -d |tar -xvf - 使用bzip2解压 加密你通过网络发送的数据如果你担心你在网络上发送数据的安全，你可以在发送你的数据之前用如mcrypt的工具加密。 服务端 $nc localhost 1567 | mcrypt –flush –bare -F -q -d -m ecb &gt; file.txt 使用mcrypt工具加密数据。客户端 $mcrypt –flush –bare -F -q -m ecb &lt; file.txt | nc -l 1567 使用mcrypt工具解密数据。以上两个命令会提示需要密码，确保两端使用相同的密码。 这里我们是使用mcrypt用来加密，使用其它任意加密工具都可以。 流视频虽然不是生成流视频的最好方法，但如果服务器上没有特定的工具，使用netcat，我们仍然有希望做成这件事。 服务端 $cat video.avi | nc -l 1567 这里我们只是从一个视频文件中读入并重定向输出到netcat客户端 $nc 172.31.100.7 1567 | mplayer -vo x11 -cache 3000 - 这里我们从socket中读入数据并重定向到mplayer。 克隆一个设备如果你已经安装配置一台Linux机器并且需要重复同样的操作对其他的机器，而你不想在重复配置一遍。不在需要重复配置安装的过程，只启动另一台机器的一些引导可以随身碟和克隆你的机器。 克隆Linux PC很简单，假如你的系统在磁盘/dev/sda上 Server $dd if=/dev/sda | nc -l 1567 Client $nc -n 172.31.100.7 1567 | dd of=/dev/sda dd是一个从磁盘读取原始数据的工具，我通过netcat服务器重定向它的输出流到其他机器并且写入到磁盘中，它会随着分区表拷贝所有的信息。但是如果我们已经做过分区并且只需要克隆root分区，我们可以根据我们系统root分区的位置，更改sda 为sda1，sda2.等等。 shell 和反向shell略.]]></content>
      <categories>
        <category>工具命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim 中如何替换选中行或指定几行内的文本]]></title>
    <url>%2F2017%2F05%2F20%2F2017-05-20-vim-replace-text%2F</url>
    <content type="text"><![CDATA[转载自segmentfault.com问答:vim 中如何替换选中行或指定几行内的文本 :&#39;&lt;,&#39;&gt;s/替换项/替换为/g以下命令将文中所有的字符串idiots替换成managers： :1,$s/idiots/manages/g 通常我们会在命令中使用%指代整个文件做为替换范围： :%s/search/replace/g 以下命令指定只在第5至第15行间进行替换: :5,15s/dog/cat/g 以下命令指定只在当前行至文件结尾间进行替换: :.,$s/dog/cat/g 以下命令指定只在后续9行内进行替换: :.,.+8s/dog/cat/g 你还可以将特定字符做为替换范围。比如，将SQL语句从FROM至分号部分中的所有等号（=）替换为不等号（&lt;&gt;）： :/FROM/,/;/s/=/&lt;&gt;/g 在可视化模式下，首先选择替换范围, 然后输入:进入命令模式，就可以利用s命令在选中的范围内进行文本替换。 VIM学习笔记 替换(Substitute)（可能要Anti-GFW） 正则表达式替换:替换r成ssdbr, 其中r时”^ r “与”[ r “.使用:%s/\(^ *\)\(r \)/\1ssdb\2/gc 与:%s/\(\[ *\)\(r \)/\1ssdb\2/gc]]></content>
      <categories>
        <category>工具命令</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
</search>
